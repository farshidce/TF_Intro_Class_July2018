{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The TensorFlow `Dataset` framework – main components\n",
    "\n",
    "The TensorFlow Dataset framework has two main components:\n",
    "\n",
    "* The `Dataset`\n",
    "* An associated `Iterator`\n",
    "\n",
    "The Dataset is basically where the data resides. This data can be loaded in from a number of sources – existing tensors, numpy arrays and numpy files, the `TFRecord` format and direct from text files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Dataset` structure\n",
    "\n",
    "* A dataset is set up to hold elements that each have the same structure (i.e. our data samples).\n",
    "* These elements can hold one or more `tf.Tensor` objects called _components_, each with an associated `tf.DType` and a `tf.TensorShape` which can be fully or partially specified.\n",
    "* You can use the `Dataset.output_types` and `Dataset.output_shapes` properties to check on the types and shapes of each component of a dataset element. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " <dtype: 'float32'> \n",
      "\n",
      "(10,) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset1 = tf.data.Dataset.from_tensor_slices(tf.random_uniform([4,10]))\n",
    "print('\\n',dataset1.output_types,'\\n')\n",
    "print(dataset1.output_shapes,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " (tf.float32, tf.int32) \n",
      "\n",
      "(TensorShape([]), TensorShape([Dimension(100)])) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset2 = tf.data.Dataset.from_tensor_slices(\n",
    "    (tf.random_uniform([4]),\n",
    "     tf.random_uniform([4,100], maxval=100, dtype=tf.int32)))\n",
    "print('\\n',dataset2.output_types,'\\n')\n",
    "print(dataset2.output_shapes,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the _nested structure_ of these properties map to the structure of an element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " (tf.float32, (tf.float32, tf.int32)) \n",
      "\n",
      "(TensorShape([Dimension(10)]), (TensorShape([]), TensorShape([Dimension(100)]))) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset3 = tf.data.Dataset.zip((dataset1, dataset2))\n",
    "print('\\n',dataset3.output_types,'\\n')\n",
    "print(dataset3.output_shapes,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naming components\n",
    "* To keep things organized, you may want to name each component of an element, for instance if they represent different features of a training example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " {'component b': tf.int32, 'component a': tf.float32} \n",
      "\n",
      "{'component b': TensorShape([Dimension(100)]), 'component a': TensorShape([])} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "   {\"component a\": tf.random_uniform([4]),\n",
    "    \"component b\": tf.random_uniform([4, 100], maxval=100, dtype=tf.int32)})\n",
    "print('\\n',dataset.output_types,'\\n')\n",
    "print(dataset.output_shapes,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Dataset` transformations\n",
    "\n",
    "Once you’ve loaded the data into the `Dataset` object, you can string together various operations to apply to the data, these include operations such as:\n",
    "\n",
    "* `batch()` – this allows you to consume the data from your TensorFlow Dataset in batches\n",
    "* `map()` – this allows you to transform the data using lambda statements applied to each element\n",
    "* `flat_map()` - maps across flattened dataset\n",
    "* `zip()` – this allows you to zip together different `Dataset` objects into a new Dataset, in a similar way to the Python zip function\n",
    "* `filter()` – this allows you to remove problematic data-points in your data-set, again based on some lambda function\n",
    "* `repeat()` – this operation restricts the number of times data is consumed from the Dataset before a `tf.errors.OutOfRangeError` error is thrown\n",
    "* `shuffle()` – this operation shuffles the data in the Dataset\n",
    "\n",
    "There are many other methods that the Dataset API includes – see https://www.tensorflow.org/api_docs/python/tf/data/Dataset for more details.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Dataset` transformations support datasets of any structure. When using the `Dataset.map()`, `Dataset.flat_map()`, and `Dataset.filter()` transformations, which apply a function to each element, the element structure determines the arguments of the function:\n",
    "\n",
    "`dataset1 = dataset1.map(lambda x: ...)`\n",
    "\n",
    "`dataset2 = dataset2.flat_map(lambda x, y: ...)`\n",
    "\n",
    "Note: Argument destructuring is not available in Python 3\n",
    "\n",
    "`dataset3 = dataset3.filter(lambda x, (y, z): ...)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `Iterator`\n",
    "The next component in the TensorFlow Dataset framework is the Iterator. This creates operations which can be called during the training, validation and/or testing of your model in TensorFlow. \n",
    "\n",
    "The `tf.data` API currently supports the following iterators:\n",
    "\n",
    "* **one-shot** - iterate once through a dataset - no need to initialize\n",
    "* **initializable** - allows parameterization of dataset definitiion - requires initialization\n",
    "* **reinitializable** - can be initialized from multiple different `Dataset` objects\n",
    "* **feedable** - allows you to select what `Iterator` to call and use via `feed_dict` mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-shot Iterators\n",
    "#### Using `.make_one_shot_iterator()` and `get_next`\n",
    "* A **one-shot** iterator only supports iterating once through a dataset.\n",
    "* Doesn't need explicit initialization\n",
    "* Currently only type of iterators easily usable with TF's `Estimator` class.\n",
    "* Handles almost all the cases that existing (and previous) queue-based input pipelines support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.range(100)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for i in range(100):\n",
    "        value = sess.run(next_element)\n",
    "        assert i == value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First create a dataset out of numpy ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.arange(0,10)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can create a TensorFlow Dataset object straight from the nparray using\n",
    "`from_tensor_slices()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx = tf.data.Dataset.from_tensor_slices(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The object `dx `is now a TensorFlow `Dataset` object. \n",
    "* The next step is to create an Iterator that will extract data from this dataset. \n",
    "* In the code below, the iterator is created using the method `make_one_shot_iterator()`.  The iterator arising from this method can only be initialized and run once – it can’t be re-initialized. The importance of being able to re-initialize an iterator will be explained more later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a one-shot iterator\n",
    "iterator = dx.make_one_shot_iterator()\n",
    "# extract an element\n",
    "next_element = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(sess.run(next_element))\n",
    "    print(sess.run(next_element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'IteratorGetNext_1:0' shape=() dtype=int64>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'IteratorGetNext_2:0' shape=() dtype=int64>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    },
    {
     "ename": "OutOfRangeError",
     "evalue": "End of sequence\n\t [[Node: IteratorGetNext_1 = IteratorGetNext[output_shapes=[[]], output_types=[DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator_1)]]\n\nCaused by op 'IteratorGetNext_1', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 345, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 1312, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.5/asyncio/events.py\", line 125, in _run\n    self._callback(*self._args)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 759, in _run_callback\n    ret = callback()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 536, in <lambda>\n    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-9-f919e9329b5a>\", line 4, in <module>\n    next_element = iterator.get_next()\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/data/ops/iterator_ops.py\", line 373, in get_next\n    name=name)), self._output_types,\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 1745, in iterator_get_next\n    output_shapes=output_shapes, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3414, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1740, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nOutOfRangeError (see above for traceback): End of sequence\n\t [[Node: IteratorGetNext_1 = IteratorGetNext[output_shapes=[[]], output_types=[DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator_1)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfRangeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfRangeError\u001b[0m: End of sequence\n\t [[Node: IteratorGetNext_1 = IteratorGetNext[output_shapes=[[]], output_types=[DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator_1)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOutOfRangeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-03d0c1a7151b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_element\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfRangeError\u001b[0m: End of sequence\n\t [[Node: IteratorGetNext_1 = IteratorGetNext[output_shapes=[[]], output_types=[DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator_1)]]\n\nCaused by op 'IteratorGetNext_1', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 345, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 1312, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.5/asyncio/events.py\", line 125, in _run\n    self._callback(*self._args)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 759, in _run_callback\n    ret = callback()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 536, in <lambda>\n    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-9-f919e9329b5a>\", line 4, in <module>\n    next_element = iterator.get_next()\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/data/ops/iterator_ops.py\", line 373, in get_next\n    name=name)), self._output_types,\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 1745, in iterator_get_next\n    output_shapes=output_shapes, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3414, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1740, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nOutOfRangeError (see above for traceback): End of sequence\n\t [[Node: IteratorGetNext_1 = IteratorGetNext[output_shapes=[[]], output_types=[DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator_1)]]\n"
     ]
    }
   ],
   "source": [
    "# extracts all the data and then throws an OutOfRangeError at end\n",
    "with tf.Session() as sess:\n",
    "    for i in range(11):\n",
    "        val = sess.run(next_element)\n",
    "        print(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializable Iterators\n",
    "#### Using `.make_initializable_iterator()` and `iterator.initializer`\n",
    "* An **initializable iterator** requires you to run an explicit `iterator.initializer` operation before using it. \n",
    "* In exchange for this inconvenience, it lets you parameterize the definition of the dataset, using one or more `tf.placeholder()` tensors that can be fed when you initialize the iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_value = tf.placeholder(tf.int64, shape=[])\n",
    "dataset = tf.data.Dataset.range(max_value)\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "next_element = iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize an iterator over a dataset with 10 elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(iterator.initializer, feed_dict={max_value: 10})\n",
    "    for i in range(10):\n",
    "        value = sess.run(next_element)\n",
    "        assert i == value\n",
    "        #print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we initialize the **same** iterator over the **same** dataset but now with a **different** number of elements - in this case 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(iterator.initializer, feed_dict={max_value: 100})\n",
    "    for i in range(100):\n",
    "      value = sess.run(next_element)\n",
    "      assert i == value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinitializable Iterators\n",
    "#### Using `Iterator.from_structure` and `make_initializer`\n",
    "* A **reinitializable iterator** can be initialized from multiple **different** `Dataset` objects. \n",
    "* For example, you might have a training input pipeline that uses some form of data augmentation for an image dataset, such as adding rotation or random crops or other random perturbations.\n",
    "* You may have a validation input pipeline that evaluates predictions on unmodified (i.e. un-augmented) data. \n",
    "* These pipelines will typically use **different** `Dataset` objects that have the **same structure** (i.e. the same types and compatible shapes for each component).\n",
    "*  A reinitializable iterator is defined by its structure. For example:\n",
    "\n",
    "`iterator = tf.data.Iterator.from_structure(training_dataset.output_types, training_dataset.output_shapes)`\n",
    "\n",
    "* We could use the `output_types` and `output_shapes` properties of either the `training_dataset` or the `validation_dataset` here, because their types and sizes are compatible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reinitializable iterator example\n",
    "* Here we are _transforming_ the input dataset by adding some random noise to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training and validation datasets with the same structure.\n",
    "training_dataset = tf.data.Dataset.range(100).map(\n",
    "    lambda x: x + tf.random_uniform([], -10, 10, tf.int64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, alternatively, we can define our own data transformation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_func(x):\n",
    "    return x + tf.random_uniform([], -10, 10, tf.int64)\n",
    "\n",
    "training_dataset = tf.data.Dataset.range(100).map(my_func).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset = tf.data.Dataset.range(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = tf.data.Iterator.from_structure(training_dataset.output_types,\n",
    "                                           training_dataset.output_shapes)\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "# Notice separate initializers for the training and validation sets\n",
    "training_init_op = iterator.make_initializer(training_dataset)\n",
    "validation_init_op = iterator.make_initializer(validation_dataset)\n",
    "\n",
    "# Run 20 epochs in which the training dataset is traversed, followed by the\n",
    "# validation dataset.\n",
    "with tf.Session() as sess:\n",
    "    for _ in range(20):\n",
    "      # Initialize an iterator over the training dataset.\n",
    "      sess.run(training_init_op)\n",
    "      for _ in range(100):\n",
    "        sess.run(next_element)\n",
    "\n",
    "      # Initialize an iterator over the validation dataset.\n",
    "      sess.run(validation_init_op)\n",
    "      for _ in range(50):\n",
    "        sess.run(next_element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feedable Iterators\n",
    "\n",
    "#### Using `tf.data.Iterator.from_string_handle()`\n",
    "\n",
    "* A feedable iterator can be used together with `tf.placeholder` to select what `Iterator` to use in each call to `tf.Session.run`, via the familiar `feed_dict` mechanism. \n",
    "* It offers the same functionality as a reinitializable iterator, but doesn't need you to initialize the iterator from the start of a dataset when you switch between iterators. \n",
    "* For example, using the same training and validation example from above, you can use `tf.data.Iterator.from_string_handle` to define a feedable iterator that allows you to switch between the two datasets.\n",
    "* A feedable iterator is defined by a **handle placeholder and its structure**. For example:\n",
    "\n",
    "`handle = tf.placeholder(tf.string, shape=[])`\n",
    "\n",
    "`iterator = tf.data.Iterator.from_string_handle(handle, training_dataset.output_types, training_dataset.output_shapes)`\n",
    "\n",
    "* We could use the `output_types` and `output_shapes` properties of either the\n",
    " `training_dataset` or the `validation_dataset` here, because they have identical structure.\n",
    "* Now we can create multiple different types of iterators, using, for example, `make_one_shot_iterator` or `make_initializable_iterator` and then use the `string_handle()` method with them to get tensors we can use in `feed_dict`\n",
    "* In other words, the `Iterator.string_handle()` method returns a tensor that can be evaluated and used to feed the `handle` placeholder when we put it in the feed_dict format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_func(x):\n",
    "    return x + tf.random_uniform([], -10, 10, tf.int64)\n",
    "\n",
    "# Define training and validation datasets with the same structure.\n",
    "training_dataset = tf.data.Dataset.range(100).map(my_func).repeat()\n",
    "validation_dataset = tf.data.Dataset.range(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have completed 301 steps\n"
     ]
    }
   ],
   "source": [
    "# This is our handle placeholder, with a dtype of tf.string\n",
    "handle = tf.placeholder(tf.string, shape=[])\n",
    "\n",
    "# Notice when defining the iterator we are using our handle placeholder \n",
    "# as well as our dataset dtypes and shapes\n",
    "iterator = tf.data.Iterator.from_string_handle(\n",
    "    handle, training_dataset.output_types, training_dataset.output_shapes)\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "# You can use feedable iterators with a variety of different kinds of iterators\n",
    "# (such as one-shot and initializable iterators).\n",
    "training_iterator = training_dataset.make_one_shot_iterator()\n",
    "validation_iterator = validation_dataset.make_initializable_iterator()\n",
    "\n",
    "counter = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # The `Iterator.string_handle()` method returns a tensor that can be evaluated\n",
    "    # and used to feed the `handle` placeholder when we put it in the feed_dict format.\n",
    "    training_handle = sess.run(training_iterator.string_handle())\n",
    "    validation_handle = sess.run(validation_iterator.string_handle())\n",
    "    \n",
    "    # Loop forever, alternating between training and validation.\n",
    "    while True:\n",
    "        # Run 200 steps using the training dataset. Note that the training dataset is\n",
    "        # infinite, and we resume from where we left off in the previous `while` loop\n",
    "        # iteration.\n",
    "        for _ in range(200):\n",
    "            sess.run(next_element, feed_dict={handle: training_handle})\n",
    "\n",
    "        # Run one pass over the validation dataset.\n",
    "        sess.run(validation_iterator.initializer)\n",
    "        for _ in range(50):\n",
    "            sess.run(next_element, feed_dict={handle: validation_handle})\n",
    "        \n",
    "        counter +=1\n",
    "        if counter > 300:\n",
    "            print(\"We have completed {} steps\".format(counter))\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consuming iterator data\n",
    "#### Understanding `Iterator.get_next()`\n",
    "* The `Iterator.get_next()` method returns one or more `tf.Tensor` objects (depending on how the `Dataset` was set up) that correspond to the symbolic next element of an iterator. \n",
    "* Each time these tensors are evaluated, they take the value of the next element in the underlying dataset. \n",
    "* Same as for other TensorFlow expressions, calling `Iterator.get_next()` does not immediately advance the iterator. Instead you need to fetch the resulting `tf.Tensor` objects in the context of a `tf.Session.run()` to get the next elements and advance the iterator.\n",
    "* For one-shot and initializable iterators: If the iterator reaches the end of the dataset, executing the Iterator.get_next() operation will raise a tf.errors.OutOfRangeError. After this point the iterator will be in an unusable state, and you must initialize it again if you want to use it further.\n",
    "* A common way of dealing with this is to wrap the training loop in a try-except block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "End of dataset\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(5)\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "# Typically `result` will be the output of a model, \n",
    "# or an optimizer's training operation.In this case we are just doing simple addition\n",
    "result = tf.add(next_element, next_element)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(iterator.initializer)\n",
    "    print(sess.run(result))  # ==> \"0\"\n",
    "    print(sess.run(result))  # ==> \"2\"\n",
    "    print(sess.run(result))  # ==> \"4\"\n",
    "    print(sess.run(result))  # ==> \"6\"\n",
    "    print(sess.run(result))  # ==> \"8\"\n",
    "    try:\n",
    "      sess.run(result)\n",
    "    except tf.errors.OutOfRangeError:\n",
    "      print(\"End of dataset\")  # ==> \"End of dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting data from iterators with a nested data structure\n",
    "* If each element of the dataset has a nested structure, the return value of `Iterator.get_next()` will be one or more `tf.Tensor` objects in the same nested structure.\n",
    "* Note that next1, next2, and next3 are tensors produced by the same op/node (created by `Iterator.get_next()`). Therefore, evaluating any of these tensors will advance the iterator for all components. \n",
    "* A typical consumer of an iterator will include all components in a single expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"IteratorGetNext_7:0\", shape=(10,), dtype=float32)\n",
      "Tensor(\"IteratorGetNext_7:1\", shape=(), dtype=float32)\n",
      "Tensor(\"IteratorGetNext_7:2\", shape=(100,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "dataset1 = tf.data.Dataset.from_tensor_slices(tf.random_uniform([4, 10]))\n",
    "dataset2 = tf.data.Dataset.from_tensor_slices((tf.random_uniform([4]), tf.random_uniform([4, 100])))\n",
    "dataset3 = tf.data.Dataset.zip((dataset1, dataset2))\n",
    "\n",
    "iterator = dataset3.make_initializable_iterator()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(iterator.initializer)\n",
    "    next1, (next2, next3) = iterator.get_next()\n",
    "print(next1)\n",
    "print(next2)\n",
    "print(next3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving iterator state\n",
    "#### Using `tf.contrib.data.make_saveable_from_iterator`\n",
    "* The `tf.contrib.data.make_saveable_from_iterator` function creates a `SaveableObject` from an iterator, which can be used to save and restore the current state of the iterator (and, effectively, the whole input pipeline). \n",
    "* A saveable object thus created can be added to `tf.train.Saver` variables list or the `tf.GraphKeys.SAVEABLE_OBJECTS` collection for saving and restoring in the same manner as a `tf.Variable`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "dataset = tf.data.Dataset.range(100)\n",
    "iterator_to_save = dataset.make_initializable_iterator()\n",
    "\n",
    "# Create saveable object from iterator.\n",
    "saveable = tf.contrib.data.make_saveable_from_iterator(iterator_to_save)\n",
    "\n",
    "# Save the iterator state by adding it to the saveable objects collection.\n",
    "tf.add_to_collection(tf.GraphKeys.SAVEABLE_OBJECTS, saveable)\n",
    "\n",
    "# define a saver as normal\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# you'll have some creterion for saving\n",
    "should_checkpoint=True\n",
    "path_to_checkpoint='./logs/20_Dataset_API'\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(iterator_to_save.initializer)\n",
    "    if should_checkpoint:\n",
    "        saver.save(sess, path_to_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./logs/20_Dataset_API\n"
     ]
    }
   ],
   "source": [
    "# Restore the iterator state.\n",
    "with tf.Session() as sess:\n",
    "  saver.restore(sess, path_to_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching dataset elements\n",
    "#### Simple batching: Using `.batch()`\n",
    "* The simplest form of batching stacks n consecutive elements of a dataset into a single element. \n",
    "* The `Dataset.batch()` transformation does exactly this, with the same constraints as the `tf.stack()` operator, applied to each component of the elements: i.e. for each component i, all elements must have a tensor of the exact same shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1, 2, 3]), array([ 0, -1, -2, -3]))\n",
      "(array([4, 5, 6, 7]), array([-4, -5, -6, -7]))\n",
      "(array([ 8,  9, 10, 11]), array([ -8,  -9, -10, -11]))\n"
     ]
    }
   ],
   "source": [
    "inc_dataset = tf.data.Dataset.range(100)\n",
    "dec_dataset = tf.data.Dataset.range(0, -100, -1)\n",
    "dataset = tf.data.Dataset.zip((inc_dataset, dec_dataset))\n",
    "batched_dataset = dataset.batch(4)\n",
    "\n",
    "iterator = batched_dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(next_element))  # ==> ([0, 1, 2,   3],   [ 0, -1,  -2,  -3])\n",
    "    print(sess.run(next_element))  # ==> ([4, 5, 6,   7],   [-4, -5,  -6,  -7])\n",
    "    print(sess.run(next_element))  # ==> ([8, 9, 10, 11],   [-8, -9, -10, -11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another example with `.batch()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0,10)\n",
    "dx = tf.data.Dataset.from_tensor_slices(x).batch(3)\n",
    "iterator = dx.make_initializable_iterator()\n",
    "next_element = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0, 1, 2])] 0\n",
      "[array([3, 4, 5])] 1\n",
      "[array([6, 7, 8])] 2\n",
      "[array([0, 1, 2])] 3\n",
      "[array([3, 4, 5])] 4\n",
      "[array([6, 7, 8])] 5\n",
      "[array([0, 1, 2])] 6\n",
      "[array([3, 4, 5])] 7\n",
      "[array([6, 7, 8])] 8\n",
      "[array([0, 1, 2])] 9\n",
      "[array([3, 4, 5])] 10\n",
      "[array([6, 7, 8])] 11\n",
      "[array([0, 1, 2])] 12\n",
      "[array([3, 4, 5])] 13\n",
      "[array([6, 7, 8])] 14\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(iterator.initializer)\n",
    "    for i in range(15):\n",
    "        val = sess.run(next_element)\n",
    "        print([val], i)\n",
    "        if (i + 1) % (10 // 3) == 0 and i > 0:\n",
    "            sess.run(iterator.initializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### zip datasets together to pair input-output training/validation pairs of data\n",
    "* In this example, the batching takes place appropriately whith the zipped together datasets (i.e. 3 items from dx, 3 items from dy.\n",
    "* The re-initialization `if` statement on the last two lines can be shorted by replacing the dcomb dataset creation line as seen below where adding the `.repeat()` method without argument which means the dataset can be repeated indefinitely without throwing an OutOfRangeError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0,10)\n",
    "y = np.arange(1, 11)\n",
    "\n",
    "def simple_zip_example(x, y):\n",
    "    # create dataset objects from the arrays\n",
    "    dx = tf.data.Dataset.from_tensor_slices(x)\n",
    "    dy = tf.data.Dataset.from_tensor_slices(y)\n",
    "    \n",
    "    # Zip the two datasets together\n",
    "    #dcomb = tf.data.Dataset.zip((dx, dy)).batch(3)\n",
    "    dcomb = tf.data.Dataset.zip((dx, dy)).repeat().batch(3)\n",
    "    iterator = dcomb.make_initializable_iterator()\n",
    "    \n",
    "    # extract an element\n",
    "    next_element = iterator.get_next()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(iterator.initializer)\n",
    "        for i in range(15):\n",
    "            val = sess.run(next_element)\n",
    "            print(val)\n",
    "#             if (i+1) % (10 // 3) == 0 and i > 0:\n",
    "#                sess.run(iterator.initializer)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1, 2]), array([1, 2, 3]))\n",
      "(array([3, 4, 5]), array([4, 5, 6]))\n",
      "(array([6, 7, 8]), array([7, 8, 9]))\n",
      "(array([9, 0, 1]), array([10,  1,  2]))\n",
      "(array([2, 3, 4]), array([3, 4, 5]))\n",
      "(array([5, 6, 7]), array([6, 7, 8]))\n",
      "(array([8, 9, 0]), array([ 9, 10,  1]))\n",
      "(array([1, 2, 3]), array([2, 3, 4]))\n",
      "(array([4, 5, 6]), array([5, 6, 7]))\n",
      "(array([7, 8, 9]), array([ 8,  9, 10]))\n",
      "(array([0, 1, 2]), array([1, 2, 3]))\n",
      "(array([3, 4, 5]), array([4, 5, 6]))\n",
      "(array([6, 7, 8]), array([7, 8, 9]))\n",
      "(array([9, 0, 1]), array([10,  1,  2]))\n",
      "(array([2, 3, 4]), array([3, 4, 5]))\n"
     ]
    }
   ],
   "source": [
    "simple_zip_example(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimal MNIST Dataset example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = digits[0][:int(len(digits[0]) * 0.8)]\n",
    "train_labels = digits[1][:int(len(digits[0]) * 0.8)]\n",
    "valid_images = digits[0][int(len(digits[0]) * 0.8):]\n",
    "valid_labels = digits[1][int(len(digits[0]) * 0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the training datasets\n",
    "dx_train = tf.data.Dataset.from_tensor_slices(train_images)\n",
    "# apply a one-hot transform to each label for use in the NN\n",
    "dy_train = tf.data.Dataset.from_tensor_slices(train_labels).map(lambda z: tf.one_hot(z, 10))\n",
    "# zip the x and y training_data together and shuffle, batch etc.\n",
    "train_dataset = tf.data.Dataset.zip((dx_train, dy_train)).shuffle(500).repeat().batch(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same operations for the validation set\n",
    "dx_valid = tf.data.Dataset.from_tensor_slices(valid_images)\n",
    "dy_valid = tf.data.Dataset.from_tensor_slices(valid_labels).map(lambda z: tf.one_hot(z, 10))\n",
    "valid_dataset = tf.data.Dataset.zip((dx_valid, dy_valid)).shuffle(500).repeat().batch(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now, we want to be able to extract data from either the train_dataset or the valid_dataset seamlessly. \n",
    "* This is important, as we don’t want to have to change how data flows through the neural network structure when all we want to do is just change the dataset the model is consuming. \n",
    "* To do this, we can use another way of creating the Iterator object – the `from_structure()` method. \n",
    "* This method creates a generic iterator object – all it needs is the data types of the data it will be outputting and the output data size/shape in order to be created. \n",
    "The code below uses this methodology:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create general iterator\n",
    "iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                          train_dataset.output_shapes)\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "# Now we need operations which can be called during training or eval to initialize\n",
    "# this generic iterator and \"point it\" to the desired dataset.\n",
    "training_init_op = iterator.make_initializer(train_dataset)\n",
    "validation_init_op = iterator.make_initializer(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(in_data):\n",
    "    bn = tf.layers.batch_normalization(in_data)\n",
    "    fc1 = tf.layers.dense(bn, 50)\n",
    "    fc2 = tf.layers.dense(fc1, 50)\n",
    "    fc2 = tf.layers.dropout(fc2)\n",
    "    fc3 = tf.layers.dense(fc2, 10)\n",
    "    return fc3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note that the next_element operation is handled directly in the model – in other words, it doesn’t need to be called explicitly during the training loop as will be seen below.\n",
    "* Rather, whenever any of the operations following this point in the graph are called (i.e. the loss operation, the optimization operation etc.) the TensorFlow graph structure will know to run the next_element operation and extract the data from whichever dataset has been initialized into the iterator. \n",
    "* The next_element operation, because it is operating on the generic iterator which is defined by the shape of the train_dataset, is a tuple – the first element ([0]) will contain the MNIST images, while the second element ([1]) will contain the corresponding labels. Therefore, next_element[0] will extract the image data batch and send it into the neural network model (nn_model) as the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = nn_model(next_element[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the optimizer and loss\n",
    "loss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits_v2(labels=next_element[1], logits=logits))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "# get accuracy\n",
    "prediction = tf.argmax(logits, 1)\n",
    "equality = tf.equal(prediction, tf.argmax(next_element[1], 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(equality, tf.float32))\n",
    "init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 588.366, training accuracy: 13.33%\n",
      "Epoch: 50, loss: 35.890, training accuracy: 76.67%\n",
      "Epoch: 100, loss: 16.141, training accuracy: 80.00%\n",
      "Epoch: 150, loss: 2.740, training accuracy: 96.67%\n",
      "Epoch: 200, loss: 5.205, training accuracy: 96.67%\n",
      "Epoch: 250, loss: 8.484, training accuracy: 86.67%\n",
      "Epoch: 300, loss: 2.002, training accuracy: 96.67%\n",
      "Epoch: 350, loss: 1.934, training accuracy: 100.00%\n",
      "Epoch: 400, loss: 1.203, training accuracy: 100.00%\n",
      "Epoch: 450, loss: 2.091, training accuracy: 96.67%\n",
      "Epoch: 500, loss: 0.458, training accuracy: 100.00%\n",
      "Epoch: 550, loss: 2.094, training accuracy: 93.33%\n",
      "Average validation set accuracy over 100 iterations is 88.67%\n"
     ]
    }
   ],
   "source": [
    "# run the training\n",
    "epochs = 600\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    sess.run(training_init_op)\n",
    "    for i in range(epochs):\n",
    "        l, _, acc = sess.run([loss, optimizer, accuracy])\n",
    "        if i % 50 == 0:\n",
    "            print(\"Epoch: {}, loss: {:.3f}, training accuracy: {:.2f}%\".format(i,l,acc*100))\n",
    "    # now setup the validation run\n",
    "    valid_iters = 100\n",
    "    # re-initialize the iterator, but this time with validation data\n",
    "    sess.run(validation_init_op)\n",
    "    avg_acc = 0\n",
    "    for i in range(valid_iters):\n",
    "        acc = sess.run([accuracy])\n",
    "        avg_acc += acc[0]\n",
    "    print(\"Average validation set accuracy over {} iterations is {:.2f}%\".format(valid_iters,avg_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some repetition from here on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.68805744 0.11730824]\n"
     ]
    }
   ],
   "source": [
    "x = np.random.sample((100,2))\n",
    "# make a dataset from a numpy array\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x)\n",
    "\n",
    "iter = dataset.make_one_shot_iterator()\n",
    "el = iter.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(el))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0.39828852, 0.73085527]), array([0.54407894]))\n"
     ]
    }
   ],
   "source": [
    "# using two numpy arrays\n",
    "features, labels = (np.random.sample((100,2)), np.random.sample((100,1)))\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features,labels))\n",
    "\n",
    "iter = dataset.make_one_shot_iterator()\n",
    "el = iter.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(el))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5345912  0.29279315]\n"
     ]
    }
   ],
   "source": [
    "# using a tensor \n",
    "dataset = tf.data.Dataset.from_tensor_slices(tf.random_uniform([100, 2]))\n",
    "\n",
    "iter = dataset.make_initializable_iterator()\n",
    "el = iter.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(iter.initializer)\n",
    "    print(sess.run(el))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5966419 0.5760475]\n"
     ]
    }
   ],
   "source": [
    "# using a placeholder\n",
    "x = tf.placeholder(tf.float32, shape=[None,2])\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x)\n",
    "\n",
    "data = np.random.sample((100,2))\n",
    "\n",
    "iter = dataset.make_initializable_iterator()\n",
    "el = iter.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(iter.initializer, feed_dict={ x: data })\n",
    "    print(sess.run(el))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.]\n"
     ]
    }
   ],
   "source": [
    "# from generator\n",
    "sequence = np.array([[1],[2,3],[3,4]])\n",
    "\n",
    "def generator():\n",
    "    for el in sequence:\n",
    "        yield el\n",
    "\n",
    "dataset = tf.data.Dataset().from_generator(generator,\n",
    "                                           output_types=tf.float32, \n",
    "                                           output_shapes=((1,)))\n",
    "iter = dataset.make_initializable_iterator()\n",
    "el = iter.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(iter.initializer)\n",
    "    print(sess.run(el))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1., 2.], dtype=float32), array([0.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# initializable iterator to switch between data\n",
    "EPOCHS = 10\n",
    "\n",
    "x, y = tf.placeholder(tf.float32, shape=[None,2]), tf.placeholder(tf.float32, shape=[None,1])\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "\n",
    "train_data = (np.random.sample((100,2)), np.random.sample((100,1)))\n",
    "test_data = (np.array([[1,2]]), np.array([[0]]))\n",
    "\n",
    "iter = dataset.make_initializable_iterator()\n",
    "features, labels = iter.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "#     initialise iterator with train data\n",
    "    sess.run(iter.initializer, feed_dict={ x: train_data[0], y: train_data[1]})\n",
    "    for _ in range(EPOCHS):\n",
    "        sess.run([features, labels])\n",
    "#     switch to test data\n",
    "    sess.run(iter.initializer, feed_dict={ x: test_data[0], y: test_data[1]})\n",
    "    print(sess.run([features, labels]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.75548354, 0.47333289]), array([0.36416387])]\n"
     ]
    }
   ],
   "source": [
    "# Reinitializable iterator to switch between Datasets\n",
    "EPOCHS = 10\n",
    "# making fake data using numpy\n",
    "train_data = (np.random.sample((100,2)), np.random.sample((100,1)))\n",
    "test_data = (np.random.sample((10,2)), np.random.sample((10,1)))\n",
    "# create two datasets, one for training and one for test\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_data)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(test_data)\n",
    "# create a iterator of the correct shape and type\n",
    "iter = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                           train_dataset.output_shapes)\n",
    "features, labels = iter.get_next()\n",
    "# create the initialisation operations\n",
    "train_init_op = iter.make_initializer(train_dataset)\n",
    "test_init_op = iter.make_initializer(test_dataset)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(train_init_op) # switch to train dataset\n",
    "    for _ in range(EPOCHS):\n",
    "        sess.run([features, labels])\n",
    "    sess.run(test_init_op) # switch to val dataset\n",
    "    print(sess.run([features, labels]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.76792777 0.51634994]\n",
      " [0.00132168 0.18642143]\n",
      " [0.69443948 0.63152526]\n",
      " [0.16554104 0.57096108]]\n"
     ]
    }
   ],
   "source": [
    "# BATCHING\n",
    "BATCH_SIZE = 4\n",
    "x = np.random.sample((100,2))\n",
    "# make a dataset from a numpy array\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x).batch(BATCH_SIZE)\n",
    "\n",
    "iter = dataset.make_one_shot_iterator()\n",
    "el = iter.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(el))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "# REPEAT\n",
    "BATCH_SIZE = 4\n",
    "x = np.array([[1],[2],[3],[4]])\n",
    "# make a dataset from a numpy array\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x)\n",
    "dataset = dataset.repeat()\n",
    "\n",
    "iter = dataset.make_one_shot_iterator()\n",
    "el = iter.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(el))\n",
    "#     this will run forever\n",
    "#     while True:\n",
    "#         print(sess.run(el))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n"
     ]
    }
   ],
   "source": [
    "# MAP\n",
    "x = np.array([[1],[2],[3],[4]])\n",
    "# make a dataset from a numpy array\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x)\n",
    "dataset = dataset.map(lambda x: x*2)\n",
    "\n",
    "iter = dataset.make_one_shot_iterator()\n",
    "el = iter.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(el))\n",
    "#     this will run forever\n",
    "#         for _ in range(len(x)):\n",
    "#             print(sess.run(el))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4]\n",
      " [2]\n",
      " [3]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "# SHUFFLE\n",
    "BATCH_SIZE = 4\n",
    "x = np.array([[1],[2],[3],[4]])\n",
    "# make a dataset from a numpy array\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x)\n",
    "dataset = dataset.shuffle(buffer_size=100)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "\n",
    "iter = dataset.make_one_shot_iterator()\n",
    "el = iter.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(el))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, Loss: 0.2183\n",
      "Iter: 1, Loss: 0.2084\n",
      "Iter: 2, Loss: 0.1990\n",
      "Iter: 3, Loss: 0.1901\n",
      "Iter: 4, Loss: 0.1817\n",
      "Iter: 5, Loss: 0.1738\n",
      "Iter: 6, Loss: 0.1664\n",
      "Iter: 7, Loss: 0.1595\n",
      "Iter: 8, Loss: 0.1530\n",
      "Iter: 9, Loss: 0.1471\n"
     ]
    }
   ],
   "source": [
    "# how to pass the value to a model\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 16\n",
    "# using two numpy arrays\n",
    "features, labels = (np.array([np.random.sample((100,2))]), \n",
    "                    np.array([np.random.sample((100,1))]))\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features,labels)).repeat().batch(BATCH_SIZE)\n",
    "\n",
    "iter = dataset.make_one_shot_iterator()\n",
    "x, y = iter.get_next()\n",
    "\n",
    "# make a simple model\n",
    "net = tf.layers.dense(x, 8, activation=tf.tanh) # pass the first value from iter.get_next() as input\n",
    "net = tf.layers.dense(net, 8, activation=tf.tanh)\n",
    "prediction = tf.layers.dense(net, 1, activation=tf.tanh)\n",
    "\n",
    "loss = tf.losses.mean_squared_error(prediction, y) # pass the second value from iter.get_net() as label\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(EPOCHS):\n",
    "        _, loss_value = sess.run([train_op, loss])\n",
    "        print(\"Iter: {}, Loss: {:.4f}\".format(i, loss_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Iter: 0, Loss: 0.3523\n",
      "Iter: 1, Loss: 0.2568\n",
      "Iter: 2, Loss: 0.1890\n",
      "Iter: 3, Loss: 0.1561\n",
      "Iter: 4, Loss: 0.1304\n",
      "Iter: 5, Loss: 0.1114\n",
      "Iter: 6, Loss: 0.0956\n",
      "Iter: 7, Loss: 0.0839\n",
      "Iter: 8, Loss: 0.0796\n",
      "Iter: 9, Loss: 0.0710\n",
      "Test Loss: 0.066840\n"
     ]
    }
   ],
   "source": [
    "# Wrapping all together -> Switch between train and test set\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 16\n",
    "# create a placeholder to dynamically switch between batch sizes\n",
    "batch_size = tf.placeholder(tf.int64)\n",
    "\n",
    "x, y = tf.placeholder(tf.float32, shape=[None,2]), tf.placeholder(tf.float32, shape=[None,1])\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x, y)).batch(batch_size).repeat()\n",
    "\n",
    "# using two numpy arrays\n",
    "train_data = (np.random.sample((100,2)), np.random.sample((100,1)))\n",
    "test_data = (np.random.sample((20,2)), np.random.sample((20,1)))\n",
    "\n",
    "n_batches = len(train_data[0]) // BATCH_SIZE\n",
    "\n",
    "iter = dataset.make_initializable_iterator()\n",
    "features, labels = iter.get_next()\n",
    "# make a simple model\n",
    "net = tf.layers.dense(features, 8, activation=tf.tanh) # pass the first value from iter.get_next() as input\n",
    "net = tf.layers.dense(net, 8, activation=tf.tanh)\n",
    "prediction = tf.layers.dense(net, 1, activation=tf.tanh)\n",
    "\n",
    "loss = tf.losses.mean_squared_error(prediction, labels) # pass the second value from iter.get_net() as label\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # initialise iterator with train data\n",
    "    sess.run(iter.initializer, feed_dict={ x: train_data[0], y: train_data[1], batch_size: BATCH_SIZE})\n",
    "    print('Training...')\n",
    "    for i in range(EPOCHS):\n",
    "        tot_loss = 0\n",
    "        for _ in range(n_batches):\n",
    "            _, loss_value = sess.run([train_op, loss])\n",
    "            tot_loss += loss_value\n",
    "        print(\"Iter: {}, Loss: {:.4f}\".format(i, tot_loss / n_batches))\n",
    "    # initialise iterator with test data\n",
    "    sess.run(iter.initializer, feed_dict={ x: test_data[0], y: test_data[1], batch_size: test_data[0].shape[0]})\n",
    "    print('Test Loss: {:4f}'.format(sess.run(loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consuming TFRecord data\n",
    "* The TFRecord file format is a simple record-oriented binary format that many TensorFlow applications use for training data. \\\n",
    "* The `tf.data.TFRecordDataset` class enables you to stream over the contents of one or more TFRecord files as part of an input pipeline."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Creates a dataset that reads all of the examples from two files.\n",
    "filenames = [\"/var/data/file1.tfrecord\", \"/var/data/file2.tfrecord\"]\n",
    "dataset = tf.data.TFRecordDataset(filenames)\n",
    "The filenames argument to the TFRecordDataset initializer can either be a string, a list of strings, or a tf.Tensor of strings. Therefore if you have two sets of files for training and validation purposes, you can use a tf.placeholder(tf.string) to represent the filenames, and initialize an iterator from the appropriate filenames:\n",
    "\n",
    "filenames = tf.placeholder(tf.string, shape=[None])\n",
    "dataset = tf.data.TFRecordDataset(filenames)\n",
    "dataset = dataset.map(...)  # Parse the record into tensors.\n",
    "dataset = dataset.repeat()  # Repeat the input indefinitely.\n",
    "dataset = dataset.batch(32)\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "\n",
    "# You can feed the initializer with the appropriate filenames for the current\n",
    "# phase of execution, e.g. training vs. validation.\n",
    "\n",
    "# Initialize `iterator` with training data.\n",
    "training_filenames = [\"/var/data/file1.tfrecord\", \"/var/data/file2.tfrecord\"]\n",
    "sess.run(iterator.initializer, feed_dict={filenames: training_filenames})\n",
    "\n",
    "# Initialize `iterator` with validation data.\n",
    "validation_filenames = [\"/var/data/validation1.tfrecord\", ...]\n",
    "sess.run(iterator.initializer, feed_dict={filenames: validation_filenames})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
