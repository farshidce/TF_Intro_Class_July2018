{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save & Restore with Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* During optimization we save the variables of the neural network whenever its classification accuracy has improved on the validation-set. \n",
    "* The optimization is aborted when there has been no improvement for 1000 iterations. \n",
    "* We then reload the variables that performed best on the validation-set.\n",
    "* This strategy is called Early Stopping. \n",
    "* It is used to avoid overfitting, which is not really a problem for the neural network used in this tutorial on the MNIST data-set for recognizing hand-written digits, but it demonstrates the idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flowchart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The following set of images show (roughly) how the data flows in the Convolutional Neural Network implemented in this notebook. \n",
    "* This CNN has two convolutional layers and two fully-connected layers, with the last layer being used for the final classification of the input images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "<img src='../pics/mnist_weights.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time, math, os\n",
    "from datetime import timedelta\n",
    "import data # data helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.9.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/mnist/train-images-idx3-ubyte.gz already exists\n",
      "../data/mnist/train-labels-idx1-ubyte.gz already exists\n",
      "../data/mnist/t10k-images-idx3-ubyte.gz already exists\n",
      "../data/mnist/t10k-labels-idx1-ubyte.gz already exists\n"
     ]
    }
   ],
   "source": [
    "batch_size=256\n",
    "mnist_folder = '../data/mnist'\n",
    "data.download_mnist(mnist_folder)\n",
    "train, val, test = data.read_mnist(mnist_folder, flatten=False)\n",
    "# Add validatation set into training as we don't need it for this example\n",
    "new_train_images = np.concatenate([train[0],val[0]],axis=0)\n",
    "new_train_labels = np.concatenate([train[1],val[1]],axis=0)\n",
    "train = (new_train_images,new_train_labels)\n",
    "del val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Create datasets\n",
    "* We have loaded the data as a tuple of numpy arrays\n",
    "* We put the data into a `Dataset` object using the `from_tensor_slices` method\n",
    "* We shuffle it, and then split it into batches, and also batch the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices(train)\n",
    "train_data = train_data.shuffle(10000) # if you want to shuffle your data\n",
    "train_data = train_data.batch(batch_size)\n",
    "\n",
    "test_data = tf.data.Dataset.from_tensor_slices(test)\n",
    "test_data = test_data.batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Create iterator\n",
    "* In this case, we are using `from_structure` and `make_initializer` to make our reinitializable iterator\n",
    "* We reshape the image so that it is in the correct shape expected by TensorFlow's `tf.nn.conv2d` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = tf.data.Iterator.from_structure(train_data.output_types, \n",
    "                                       train_data.output_shapes)\n",
    "next_img, next_label = iterator.get_next()\n",
    "\n",
    "# reshape the image to make it work with tf.nn.conv2d\n",
    "next_img = tf.reshape(next_img, shape=[-1, 28, 28, 1])\n",
    "\n",
    "train_init = iterator.make_initializer(train_data)  # initializer for train_data\n",
    "test_init = iterator.make_initializer(test_data)    # initializer for test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST data-set has now been loaded and consists of 70,000 images and associated labels (i.e. classifications of the images). The data-set is split into 3 mutually exclusive sub-sets. We will only use the training and test-sets in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of:\n",
      "- Training-set:\t\t(60000, 28, 28)\n",
      "- Test-set:\t\t(10000, 28, 28)\n",
      "- Training-labels:\t\t(60000, 10)\n",
      "- Test-set-labels:\t\t(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of:\")\n",
    "print(\"- Training-set:\\t\\t{}\".format(train[0].shape))\n",
    "print(\"- Test-set:\\t\\t{}\".format(test[0].shape))\n",
    "print(\"- Training-labels:\\t\\t{}\".format(train[1].shape))\n",
    "print(\"- Test-set-labels:\\t\\t{}\".format(test[1].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class-labels are One-Hot encoded, which means that each label is a vector with 10 elements, all of which are zero except for one element. The index of this one element is the class-number, that is, the digit shown in the associated image. We also need the class-numbers as integers for the test- and validation-sets, so we calculate them now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_numbers = np.argmax(test[1], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data dimensions are used in several places in the source-code below. They are defined once so we can use these variables instead of numbers throughout the source-code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We know that MNIST images are 28 pixels in each dimension.\n",
    "img_size = 28\n",
    "\n",
    "# Images are stored in one-dimensional arrays of this length.\n",
    "img_size_flat = img_size * img_size\n",
    "\n",
    "# Tuple with height and width of images used to reshape arrays.\n",
    "img_shape = (img_size, img_size)\n",
    "\n",
    "# Number of colour channels for the images: 1 channel for gray-scale.\n",
    "num_channels = 1\n",
    "\n",
    "# Number of classes, one class for each of 10 digits.\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper-function for plotting images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function used to plot 9 images in a 3x3 grid, and writing the true and predicted classes below each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images, cls_true, cls_pred=None):\n",
    "    assert len(images) == len(cls_true) == 9\n",
    "    \n",
    "    # Create figure with 3x3 sub-plots.\n",
    "    fig, axes = plt.subplots(3, 3,figsize=(6,6))\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Plot image.\n",
    "        ax.imshow(images[i].reshape(img_shape), cmap='binary')\n",
    "\n",
    "        # Show true and predicted classes. \n",
    "        # We convert from one-hot to integer labels using np.argmax\n",
    "        if cls_pred is None:\n",
    "            xlabel = \"True: {0}\".format(np.argmax(cls_true[i]))\n",
    "        else:\n",
    "            xlabel = \"True: {0}, Pred: {1}\".format(np.argmax(cls_true[i]), cls_pred[i])\n",
    "\n",
    "        # Show the classes as the label on the x-axis.\n",
    "        ax.set_xlabel(xlabel)\n",
    "        \n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    \n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot a few images to see if data is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the first images from the test-set.\n",
    "images = test[0][0:9]\n",
    "\n",
    "# Get the true classes for those images.\n",
    "cls_true = test[1][0:9]\n",
    "\n",
    "# Plot the images and labels using our helper-function above.\n",
    "plot_images(images=images, cls_true=cls_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholder variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Placeholder variables serve as the input to the TensorFlow computational graph that we may change each time we execute the graph. We call this feeding the placeholder variables and it is demonstrated further below.\n",
    "\n",
    "First we define the placeholder variable for the input images. This allows us to change the images that are input to the TensorFlow graph. This is a so-called tensor, which just means that it is a multi-dimensional array. The data-type is set to `float32` and the shape is set to `[None, img_size_flat]`, where `None` means that the tensor may hold an arbitrary number of images with each image being a vector of length `img_size_flat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, img_size_flat], name='x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convolutional layers expect `x` to be encoded as a 4-dim tensor so we have to reshape it so its shape is instead `[num_images, img_height, img_width, num_channels]`. Note that `img_height == img_width == img_size` and `num_images` can be inferred automatically by using -1 for the size of the first dimension. So the reshape operation is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_image = tf.reshape(x, [-1, img_size, img_size, num_channels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we have the placeholder variable for the true labels associated with the images that were input in the placeholder variable `x`. The shape of this placeholder variable is `[None, num_classes]` which means it may hold an arbitrary number of labels and each label is a vector of length `num_classes` which is 10 in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = tf.placeholder(tf.float32, shape=[None, 10], name='y_true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also have a placeholder variable for the class-number, but we will instead calculate it using argmax. Note that this is a TensorFlow operator so nothing is calculated at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_cls = tf.argmax(y_true, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network\n",
    "* First we create a series of functions to manage the main operations of our neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_activ(inputs, filters, k_size, stride, padding='VALID', scope_name='conv', activation=tf.nn.relu):\n",
    "    '''\n",
    "    A method that does convolution + an activation function on inputs\n",
    "    '''\n",
    "    with tf.variable_scope(scope_name, reuse=tf.AUTO_REUSE) as scope:\n",
    "        in_channels = inputs.shape[-1]\n",
    "        kernel = tf.get_variable('kernel', \n",
    "                                [k_size, k_size, in_channels, filters], \n",
    "                                initializer=tf.truncated_normal_initializer())\n",
    "        biases = tf.get_variable('biases', \n",
    "                                [filters],\n",
    "                                initializer=tf.random_normal_initializer())\n",
    "        conv = tf.nn.conv2d(inputs, kernel, strides=[1, stride, stride, 1], padding=padding)\n",
    "    return activation(conv + biases, name=scope.name)\n",
    "\n",
    "def maxpool(inputs, ksize, stride, padding='VALID', scope_name='pool'):\n",
    "    '''A method that does max pooling on inputs'''\n",
    "    with tf.variable_scope(scope_name, reuse=tf.AUTO_REUSE) as scope:\n",
    "        pool = tf.nn.max_pool(inputs, \n",
    "                            ksize=[1, ksize, ksize, 1], \n",
    "                            strides=[1, stride, stride, 1],\n",
    "                            padding=padding)\n",
    "    return pool\n",
    "\n",
    "def fully_connected(inputs, out_dim, scope_name='fc'):\n",
    "    '''\n",
    "    A fully connected linear layer on inputs\n",
    "    '''\n",
    "    with tf.variable_scope(scope_name, reuse=tf.AUTO_REUSE) as scope:\n",
    "        in_dim = inputs.shape[-1]\n",
    "        w = tf.get_variable('weights', [in_dim, out_dim],\n",
    "                            initializer=tf.truncated_normal_initializer())\n",
    "        b = tf.get_variable('biases', [out_dim],\n",
    "                            initializer=tf.constant_initializer(0.0))\n",
    "        out = tf.matmul(inputs, w) + b\n",
    "    return out\n",
    "\n",
    "def flatten_layer(inputs):\n",
    "    feature_dim = inputs.shape[1] * inputs.shape[2] * inputs.shape[3]\n",
    "    return tf.reshape(inputs, [-1, feature_dim])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the computational graph for our Convolutional Network architecture using our pre-defined layer functions\n",
    "* Here we are simply feeding the output of the previous layer function into the next layer function in a sequential fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes=10\n",
    "conv1 = conv_activ(inputs=next_img,filters=16,k_size=5,stride=1,scope_name='conv1')\n",
    "pool1 = maxpool(conv1, ksize=2, stride=2, padding='VALID',scope_name='pool1')\n",
    "conv2 = conv_activ(inputs=pool1,filters=36, k_size=2, stride=1,scope_name='conv2')\n",
    "pool2 = maxpool(conv2, ksize=2, stride=2, padding='VALID', scope_name='pool2')\n",
    "flattened = flatten_layer(pool2)\n",
    "fc = fully_connected(flattened, out_dim=1024, scope_name='fc')\n",
    "dropout = tf.nn.dropout(tf.nn.relu(fc), keep_prob=0.5, name='relu_dropout')\n",
    "logits = fully_connected(dropout, out_dim=n_classes, scope_name='logits')\n",
    "y_pred = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further below, we want to plot the weights of the neural network. When the network is constructed using Pretty Tensor, all the variables of the layers are created indirectly by Pretty Tensor. We therefore have to retrieve the variables from TensorFlow.\n",
    "\n",
    "We used the names `conv1` and `conv2` for the two convolutional layers. These are also called variable scopes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conv1/biases': <tf.Variable 'conv1/biases:0' shape=(16,) dtype=float32_ref>,\n",
       " 'conv1/kernel': <tf.Variable 'conv1/kernel:0' shape=(5, 5, 1, 16) dtype=float32_ref>,\n",
       " 'conv2/biases': <tf.Variable 'conv2/biases:0' shape=(36,) dtype=float32_ref>,\n",
       " 'conv2/kernel': <tf.Variable 'conv2/kernel:0' shape=(2, 2, 16, 36) dtype=float32_ref>,\n",
       " 'fc/biases': <tf.Variable 'fc/biases:0' shape=(1024,) dtype=float32_ref>,\n",
       " 'fc/weights': <tf.Variable 'fc/weights:0' shape=(900, 1024) dtype=float32_ref>,\n",
       " 'logits/biases': <tf.Variable 'logits/biases:0' shape=(10,) dtype=float32_ref>,\n",
       " 'logits/weights': <tf.Variable 'logits/weights:0' shape=(1024, 10) dtype=float32_ref>}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display layers\n",
    "layers = {v.op.name: v for v in tf.trainable_variables()}\n",
    "layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'conv1/kernel:0' shape=(5, 5, 1, 16) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('conv1', reuse=tf.AUTO_REUSE):\n",
    "    weights = tf.get_variable('kernel')\n",
    "    print(weights)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation is somewhat awkward because we have to use the TensorFlow function `get_variable()` which was designed for another purpose; either creating a new variable or re-using an existing variable. The easiest thing is to make the following helper-function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights_variable(layer_name):\n",
    "    # Retrieve an existing variable named 'kernel' in the scope with the given layer_name.\n",
    "    # This is awkward because the TensorFlow function was really intended for another purpose.\n",
    "    with tf.variable_scope(layer_name, reuse=tf.AUTO_REUSE):\n",
    "        variable = tf.get_variable('kernel')\n",
    "    return variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this helper-function we can retrieve the variables. These are TensorFlow objects. In order to get the contents of the variables, you must do something like: `contents = session.run(weights_conv1)` as demonstrated further below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_conv1 = get_weights_variable(layer_name='conv1')\n",
    "weights_conv2 = get_weights_variable(layer_name='conv2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('loss'):\n",
    "    entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=next_label, logits=logits)\n",
    "    loss = tf.reduce_mean(entropy, name='loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the predicted class-label (`y_pred`) as well as a loss-measure that must be minimized, so as to improve the ability of the neural network to classify the input images.\n",
    "\n",
    "Note that optimization is not performed at this point. In fact, nothing is calculated at all, we just add the optimizer-object to the TensorFlow graph for later execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Measures\n",
    "\n",
    "We need a few more performance measures to display the progress to the user.\n",
    "\n",
    "First we calculate the predicted class number from the output of the neural network `y_pred`, which is a vector with 10 elements. The class number is the index of the largest element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_cls = tf.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create a vector of booleans telling us whether the predicted class equals the true class of each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(y_pred_cls, y_true_cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification accuracy is calculated by first type-casting the vector of booleans to floats, so that False becomes 0 and True becomes 1, and then taking the average of these numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saver\n",
    "\n",
    "In order to save the variables of the neural network, we now create a so-called Saver-object which is used for storing and retrieving all the variables of the TensorFlow graph. Nothing is actually saved at this point, which will be done further below in the `optimize()`-function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The saved files are often called checkpoints because they may be written at regular intervals during optimization.\n",
    "\n",
    "This is the directory used for saving and retrieving the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = 'checkpoints/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the directory if it does not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the path for the checkpoint-file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join(save_dir, 'best_validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Run\n",
    "\n",
    "* Just this time, we're going to create a TensorFlow session and leave it open for the duration of this notebook\n",
    "* Normally, you should not do this, and instead encapsulate the session run in the training loop as illustrated in the other notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we also define a helper function to initialize variables\n",
    "def init_variables():\n",
    "    sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper-function to perform optimization iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 55,000 images in the training-set. It takes a long time to calculate the gradient of the model using all these images. We therefore only use a small batch of images in each iteration of the optimizer.\n",
    "\n",
    "If your computer crashes or becomes very slow because you run out of RAM, then you may try and lower this number, but you may then need to perform more optimization iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification accuracy for the validation-set will be calculated for every 100 iterations of the optimization function below. The optimization will be stopped if the validation accuracy has not been improved in 1000 iterations. We need a few variables to keep track of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best validation accuracy seen so far.\n",
    "best_validation_accuracy = 0.0\n",
    "\n",
    "# Iteration-number for last improvement to validation accuracy.\n",
    "last_improvement = 0\n",
    "\n",
    "# Stop optimization if no improvement found in this many iterations.\n",
    "require_improvement = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('predict'):\n",
    "    preds = tf.nn.softmax(logits)\n",
    "    correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(next_label, 1))\n",
    "    accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_once(sess, init, writer=None, epoch=0, step=None):\n",
    "    global training\n",
    "    n_test=10000\n",
    "    start_time = time.time()\n",
    "    sess.run(init)\n",
    "    training = False\n",
    "    total_correct_preds = 0\n",
    "    try:\n",
    "        while True:\n",
    "            accuracy_batch, summaries = sess.run([accuracy, summary_op])\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, global_step=step)\n",
    "            total_correct_preds += accuracy_batch\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        pass\n",
    "    \n",
    "    # \n",
    "    print('Accuracy at epoch {0}: {1} '.format(epoch, total_correct_preds/n_test))\n",
    "    print('Took: {0} seconds'.format(time.time() - start_time))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summaries for TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('summaries'):\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    tf.summary.histogram('histogram_loss', loss)\n",
    "    summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(sess, saver, init, writer, epoch, step, skip_step=1):\n",
    "    global training\n",
    "    start_time = time.time()\n",
    "    sess.run(init) \n",
    "    training = True\n",
    "    total_loss = 0\n",
    "    n_batches = 0\n",
    "    try:\n",
    "        while True:\n",
    "            _, l, summaries = sess.run([optimizer, loss, summary_op])\n",
    "            writer.add_summary(summaries, global_step=step)\n",
    "            if step % skip_step == 0:\n",
    "                print('Loss at step {0}: {1}'.format(step, l))\n",
    "            step += 1\n",
    "            total_loss += l\n",
    "            n_batches += 1\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        pass\n",
    "    saver.save(sess, 'checkpoints/convnet_mnist/mnist-convnet', step)\n",
    "    print('Average loss at epoch {0}: {1}'.format(epoch, total_loss/n_batches))\n",
    "    print('Took: {0} seconds'.format(time.time() - start_time))\n",
    "    return step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for performing a number of optimization iterations so as to gradually improve the variables of the network layers. In each iteration, a new batch of data is selected from the training-set and then TensorFlow executes the optimizer using those training samples.  The progress is printed every 100 iterations where the validation accuracy is also calculated and saved to a file if it is an improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a global step counter\n",
    "gstep = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(n_epochs=20,skip_step=100,restore=False,lr=1e-4,eval_init=test_init):\n",
    "    '''\n",
    "    The train function alternates between training one epoch and evaluating\n",
    "    '''\n",
    "    data.safe_mkdir('checkpoints')\n",
    "    data.safe_mkdir('checkpoints/convnet_mnist')\n",
    "    writer = tf.summary.FileWriter('./graphs/convnet', tf.get_default_graph())\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/convnet_mnist/checkpoint'))\n",
    "    if restore and ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
    "\n",
    "    step = gstep.eval(session=sess)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        step = train_one_epoch(sess, saver, train_init, writer, epoch, step, skip_step)\n",
    "        eval_once(sess, eval_init, writer, epoch, step)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 0: 14195.6943359375\n",
      "Average loss at epoch 0: 8093.8193338597075\n",
      "Took: 3.29201340675354 seconds\n",
      "Accuracy at epoch 0: 0.2356 \n",
      "Took: 0.3178446292877197 seconds\n",
      "Average loss at epoch 1: 4145.188539935173\n",
      "Took: 1.5674607753753662 seconds\n",
      "Accuracy at epoch 1: 0.4008 \n",
      "Took: 0.09257626533508301 seconds\n",
      "Average loss at epoch 2: 2481.7674207322143\n",
      "Took: 1.5850651264190674 seconds\n",
      "Accuracy at epoch 2: 0.5406 \n",
      "Took: 0.09958863258361816 seconds\n",
      "Average loss at epoch 3: 1634.4114683759974\n",
      "Took: 1.5619940757751465 seconds\n",
      "Accuracy at epoch 3: 0.6352 \n",
      "Took: 0.09436297416687012 seconds\n",
      "Loss at step 1000: 1307.5313720703125\n",
      "Average loss at epoch 4: 1172.873527883976\n",
      "Took: 1.5725932121276855 seconds\n",
      "Accuracy at epoch 4: 0.6908 \n",
      "Took: 0.08532309532165527 seconds\n",
      "Average loss at epoch 5: 892.0242977061171\n",
      "Took: 1.570378065109253 seconds\n",
      "Accuracy at epoch 5: 0.7359 \n",
      "Took: 0.0987699031829834 seconds\n",
      "Average loss at epoch 6: 705.5035489995429\n",
      "Took: 1.5529272556304932 seconds\n",
      "Accuracy at epoch 6: 0.7752 \n",
      "Took: 0.09124898910522461 seconds\n",
      "Average loss at epoch 7: 579.433004046501\n",
      "Took: 1.6068086624145508 seconds\n",
      "Accuracy at epoch 7: 0.7986 \n",
      "Took: 0.09020781517028809 seconds\n",
      "Loss at step 2000: 506.67425537109375\n",
      "Average loss at epoch 8: 480.68784880942485\n",
      "Took: 1.5977091789245605 seconds\n",
      "Accuracy at epoch 8: 0.8166 \n",
      "Took: 0.09647083282470703 seconds\n",
      "Average loss at epoch 9: 418.57891813237615\n",
      "Took: 1.6049749851226807 seconds\n",
      "Accuracy at epoch 9: 0.8364 \n",
      "Took: 0.09383845329284668 seconds\n",
      "Average loss at epoch 10: 359.6140564614154\n",
      "Took: 1.580536127090454 seconds\n",
      "Accuracy at epoch 10: 0.8455 \n",
      "Took: 0.0970766544342041 seconds\n",
      "Average loss at epoch 11: 318.0579816128345\n",
      "Took: 1.6079106330871582 seconds\n",
      "Accuracy at epoch 11: 0.8491 \n",
      "Took: 0.09474039077758789 seconds\n",
      "Loss at step 3000: 332.19049072265625\n",
      "Average loss at epoch 12: 286.77542990826544\n",
      "Took: 1.6245677471160889 seconds\n",
      "Accuracy at epoch 12: 0.8683 \n",
      "Took: 0.09700465202331543 seconds\n",
      "Average loss at epoch 13: 257.1601347578333\n",
      "Took: 1.6059014797210693 seconds\n",
      "Accuracy at epoch 13: 0.8763 \n",
      "Took: 0.09103798866271973 seconds\n",
      "Average loss at epoch 14: 227.35040477996176\n",
      "Took: 1.604475736618042 seconds\n",
      "Accuracy at epoch 14: 0.8804 \n",
      "Took: 0.10018634796142578 seconds\n",
      "Average loss at epoch 15: 210.37588965233337\n",
      "Took: 1.5989787578582764 seconds\n",
      "Accuracy at epoch 15: 0.8865 \n",
      "Took: 0.09233522415161133 seconds\n",
      "Average loss at epoch 16: 188.5346919120626\n",
      "Took: 1.6176612377166748 seconds\n",
      "Accuracy at epoch 16: 0.8865 \n",
      "Took: 0.09614205360412598 seconds\n",
      "Loss at step 4000: 182.03982543945312\n",
      "Average loss at epoch 17: 168.83871317112698\n",
      "Took: 1.616431713104248 seconds\n",
      "Accuracy at epoch 17: 0.8977 \n",
      "Took: 0.09268522262573242 seconds\n",
      "Average loss at epoch 18: 154.65158764453645\n",
      "Took: 1.6091597080230713 seconds\n",
      "Accuracy at epoch 18: 0.9026 \n",
      "Took: 0.08831930160522461 seconds\n",
      "Average loss at epoch 19: 146.04738162426239\n",
      "Took: 1.5832087993621826 seconds\n",
      "Accuracy at epoch 19: 0.9009 \n",
      "Took: 0.09715390205383301 seconds\n",
      "Average loss at epoch 20: 134.76553459979118\n",
      "Took: 1.5940954685211182 seconds\n",
      "Accuracy at epoch 20: 0.9041 \n",
      "Took: 0.09093785285949707 seconds\n",
      "Loss at step 5000: 131.67196655273438\n",
      "Average loss at epoch 21: 124.64873124792221\n",
      "Took: 1.599076271057129 seconds\n",
      "Accuracy at epoch 21: 0.9099 \n",
      "Took: 0.09660458564758301 seconds\n",
      "Average loss at epoch 22: 115.25114371117125\n",
      "Took: 1.608581781387329 seconds\n",
      "Accuracy at epoch 22: 0.9088 \n",
      "Took: 0.09153199195861816 seconds\n",
      "Average loss at epoch 23: 107.55268652084025\n",
      "Took: 1.6204943656921387 seconds\n",
      "Accuracy at epoch 23: 0.911 \n",
      "Took: 0.0959160327911377 seconds\n",
      "Average loss at epoch 24: 99.60614643502743\n",
      "Took: 1.6239454746246338 seconds\n",
      "Accuracy at epoch 24: 0.9152 \n",
      "Took: 0.08945703506469727 seconds\n",
      "Loss at step 6000: 125.27005004882812\n",
      "Average loss at epoch 25: 92.88040930565367\n",
      "Took: 1.5653324127197266 seconds\n",
      "Accuracy at epoch 25: 0.9169 \n",
      "Took: 0.09620833396911621 seconds\n",
      "Average loss at epoch 26: 89.43381151239923\n",
      "Took: 1.6210153102874756 seconds\n",
      "Accuracy at epoch 26: 0.92 \n",
      "Took: 0.09422159194946289 seconds\n",
      "Average loss at epoch 27: 82.37339774598466\n",
      "Took: 1.5965344905853271 seconds\n",
      "Accuracy at epoch 27: 0.9196 \n",
      "Took: 0.09384727478027344 seconds\n",
      "Average loss at epoch 28: 78.83008810814391\n",
      "Took: 1.6192634105682373 seconds\n",
      "Accuracy at epoch 28: 0.9262 \n",
      "Took: 0.0962071418762207 seconds\n",
      "Loss at step 7000: 30.55239486694336\n",
      "Average loss at epoch 29: 72.66083908081055\n",
      "Took: 1.599208116531372 seconds\n",
      "Accuracy at epoch 29: 0.9231 \n",
      "Took: 0.09747600555419922 seconds\n",
      "Average loss at epoch 30: 69.80453891348331\n",
      "Took: 1.6140005588531494 seconds\n",
      "Accuracy at epoch 30: 0.9289 \n",
      "Took: 0.09776973724365234 seconds\n",
      "Average loss at epoch 31: 65.2654076190705\n",
      "Took: 1.6103687286376953 seconds\n",
      "Accuracy at epoch 31: 0.9247 \n",
      "Took: 0.09093785285949707 seconds\n",
      "Average loss at epoch 32: 62.25537684014503\n",
      "Took: 1.6130223274230957 seconds\n",
      "Accuracy at epoch 32: 0.9299 \n",
      "Took: 0.0939340591430664 seconds\n",
      "Average loss at epoch 33: 57.89629850184664\n",
      "Took: 1.6040725708007812 seconds\n",
      "Accuracy at epoch 33: 0.9282 \n",
      "Took: 0.09030461311340332 seconds\n",
      "Loss at step 8000: 87.89930725097656\n",
      "Average loss at epoch 34: 54.521021238286444\n",
      "Took: 1.5547349452972412 seconds\n",
      "Accuracy at epoch 34: 0.9281 \n",
      "Took: 0.08930397033691406 seconds\n",
      "Average loss at epoch 35: 52.00777219812921\n",
      "Took: 1.6044905185699463 seconds\n",
      "Accuracy at epoch 35: 0.9296 \n",
      "Took: 0.09892845153808594 seconds\n",
      "Average loss at epoch 36: 50.12022084581091\n",
      "Took: 1.6185429096221924 seconds\n",
      "Accuracy at epoch 36: 0.9308 \n",
      "Took: 0.09648418426513672 seconds\n",
      "Average loss at epoch 37: 48.625391444754094\n",
      "Took: 1.6215498447418213 seconds\n",
      "Accuracy at epoch 37: 0.9323 \n",
      "Took: 0.09544014930725098 seconds\n",
      "Loss at step 9000: 38.20342254638672\n",
      "Average loss at epoch 38: 43.52689492651757\n",
      "Took: 1.6123113632202148 seconds\n",
      "Accuracy at epoch 38: 0.9352 \n",
      "Took: 0.10040020942687988 seconds\n",
      "Average loss at epoch 39: 42.11802692819149\n",
      "Took: 1.5926077365875244 seconds\n",
      "Accuracy at epoch 39: 0.932 \n",
      "Took: 0.09505319595336914 seconds\n",
      "Average loss at epoch 40: 40.74666147434965\n",
      "Took: 1.6074492931365967 seconds\n",
      "Accuracy at epoch 40: 0.9293 \n",
      "Took: 0.09877562522888184 seconds\n",
      "Average loss at epoch 41: 40.06873350752161\n",
      "Took: 1.6231050491333008 seconds\n",
      "Accuracy at epoch 41: 0.9369 \n",
      "Took: 0.09145092964172363 seconds\n",
      "Loss at step 10000: 26.075103759765625\n",
      "Average loss at epoch 42: 36.80218108562713\n",
      "Took: 1.6060187816619873 seconds\n",
      "Accuracy at epoch 42: 0.9375 \n",
      "Took: 0.0966188907623291 seconds\n",
      "Average loss at epoch 43: 35.09019361861208\n",
      "Took: 1.6070573329925537 seconds\n",
      "Accuracy at epoch 43: 0.938 \n",
      "Took: 0.09798431396484375 seconds\n",
      "Average loss at epoch 44: 34.208044149520546\n",
      "Took: 1.6033613681793213 seconds\n",
      "Accuracy at epoch 44: 0.938 \n",
      "Took: 0.09510517120361328 seconds\n",
      "Average loss at epoch 45: 32.899447321384514\n",
      "Took: 1.609717607498169 seconds\n",
      "Accuracy at epoch 45: 0.9339 \n",
      "Took: 0.0966806411743164 seconds\n",
      "Loss at step 11000: 17.410913467407227\n",
      "Average loss at epoch 46: 32.05639319115497\n",
      "Took: 1.5908920764923096 seconds\n",
      "Accuracy at epoch 46: 0.9393 \n",
      "Took: 0.09281301498413086 seconds\n",
      "Average loss at epoch 47: 30.40169561264363\n",
      "Took: 1.5876646041870117 seconds\n",
      "Accuracy at epoch 47: 0.9405 \n",
      "Took: 0.0932760238647461 seconds\n",
      "Average loss at epoch 48: 28.30155918445993\n",
      "Took: 1.5880954265594482 seconds\n",
      "Accuracy at epoch 48: 0.9393 \n",
      "Took: 0.09505772590637207 seconds\n",
      "Average loss at epoch 49: 27.985854731214808\n",
      "Took: 1.6237401962280273 seconds\n",
      "Accuracy at epoch 49: 0.9399 \n",
      "Took: 0.09359359741210938 seconds\n"
     ]
    }
   ],
   "source": [
    "train_network(n_epochs=10, skip_step=1000, restore=False, lr=1e-4, eval_init=test_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counter for total number of iterations performed so far.\n",
    "total_iterations = 0\n",
    "\n",
    "def optimize(num_iterations):\n",
    "    # Ensure we update the global variables rather than local copies.\n",
    "    global total_iterations\n",
    "    global best_validation_accuracy\n",
    "    global last_improvement\n",
    "\n",
    "    # Start-time used for printing time-usage below.\n",
    "    start_time = time.time()\n",
    "    sess.run(train_init)\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "\n",
    "        # Increase the total number of iterations performed.\n",
    "        # It is easier to update it in each iteration because\n",
    "        # we need this number several times in the following.\n",
    "        total_iterations += 1\n",
    "\n",
    "        try:\n",
    "            sess.run([loss,optimizer])\n",
    "        except tf.errors.OutOfRangeError or ValueError:\n",
    "            pass\n",
    "        \n",
    "        # Print status every 100 iterations and after last iteration.\n",
    "        if (total_iterations % 100 == 0) or (i == (num_iterations - 1)):\n",
    "\n",
    "            # Calculate the accuracy on the training-batch.\n",
    "            #acc_train = session.run(accuracy, feed_dict=feed_dict_train)\n",
    "            try:\n",
    "                acc_train = sess.run(accuracy)\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                pass\n",
    "\n",
    "            # Calculate the accuracy on the validation-set.\n",
    "            # The function returns 2 values but we only need the first.\n",
    "            acc_validation, _ = validation_accuracy()\n",
    "\n",
    "            # If validation accuracy is an improvement over best-known.\n",
    "            if acc_validation > best_validation_accuracy:\n",
    "                # Update the best-known validation accuracy.\n",
    "                best_validation_accuracy = acc_validation\n",
    "                \n",
    "                # Set the iteration for the last improvement to current.\n",
    "                last_improvement = total_iterations\n",
    "\n",
    "                # Save all variables of the TensorFlow graph to file.\n",
    "                saver.save(sess=sess, save_path=save_path)\n",
    "\n",
    "                # A string to be printed below, shows improvement found.\n",
    "                improved_str = '*'\n",
    "            else:\n",
    "                # An empty string to be printed below.\n",
    "                # Shows that no improvement was found.\n",
    "                improved_str = ''\n",
    "            \n",
    "            # Status-message for printing.\n",
    "            msg = \"Iter: {0:>6}, Train-Batch Accuracy: {1:>6.1%}, Validation Acc: {2:>6.1%} {3}\"\n",
    "\n",
    "            # Print it.\n",
    "            print(msg.format(i + 1, acc_train, acc_validation, improved_str))\n",
    "\n",
    "        # If no improvement found in the required number of iterations.\n",
    "        if total_iterations - last_improvement > require_improvement:\n",
    "            print(\"No improvement found in a while, stopping optimization.\")\n",
    "\n",
    "            # Break out from the for-loop.\n",
    "            break\n",
    "\n",
    "    # Ending time.\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Difference between start and end-times.\n",
    "    time_dif = end_time - start_time\n",
    "\n",
    "    # Print the time-usage.\n",
    "    print(\"Time usage: \" + str(timedelta(seconds=int(round(time_dif)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper-function for plotting convolutional weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_conv_weights(weights, input_channel=0):\n",
    "    # Assume weights are TensorFlow ops for 4-dim variables\n",
    "    # e.g. weights_conv1 or weights_conv2.\n",
    "\n",
    "    # Retrieve the values of the weight-variables from TensorFlow.\n",
    "    # A feed-dict is not necessary because nothing is calculated.\n",
    "    w = sess.run(weights)\n",
    "\n",
    "    # Print mean and standard deviation.\n",
    "    print(\"Mean: {0:.5f}, Stdev: {1:.5f}\".format(w.mean(), w.std()))\n",
    "    \n",
    "    # Get the lowest and highest values for the weights.\n",
    "    # This is used to correct the colour intensity across\n",
    "    # the images so they can be compared with each other.\n",
    "    w_min = np.min(w)\n",
    "    w_max = np.max(w)\n",
    "\n",
    "    # Number of filters used in the conv. layer.\n",
    "    num_filters = w.shape[3]\n",
    "\n",
    "    # Number of grids to plot.\n",
    "    # Rounded-up, square-root of the number of filters.\n",
    "    num_grids = math.ceil(math.sqrt(num_filters))\n",
    "    \n",
    "    # Create figure with a grid of sub-plots.\n",
    "    fig, axes = plt.subplots(num_grids, num_grids)\n",
    "\n",
    "    # Plot all the filter-weights.\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Only plot the valid filter-weights.\n",
    "        if i<num_filters:\n",
    "            # Get the weights for the i'th filter of the input channel.\n",
    "            # The format of this 4-dim tensor is determined by the\n",
    "            # TensorFlow API. See Tutorial #02 for more details.\n",
    "            img = w[:, :, input_channel, i]\n",
    "\n",
    "            # Plot image.\n",
    "            ax.imshow(img, vmin=w_min, vmax=w_max,\n",
    "                      interpolation='nearest', cmap='seismic')\n",
    "        \n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    \n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show convolution weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at epoch 99999999999: 0.9391 \n",
      "Took: 0.08668327331542969 seconds\n"
     ]
    }
   ],
   "source": [
    "eval_once(sess, test_init, writer=None, epoch=99999999999, step=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.01481, Stdev: 0.83997\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_conv_weights(weights=weights_conv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close the Graph Session\n",
    "* We need to remember to do this to get TensorFlow to release system resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "sess.close()\n",
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
