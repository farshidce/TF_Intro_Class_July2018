{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Types of Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dilated Convolutions\n",
    "\n",
    "* Dilated convolutions introduce another parameter to convolutional layers called the **dilation rate**. \n",
    "* This defines a spacing between the values in a kernel. \n",
    "* A 3x3 kernel with a dilation rate of 2 will have the same field of view as a 5x5 kernel, while only using 9 parameters. \n",
    "* Imagine taking a 5x5 kernel and deleting every second column and row.\n",
    "* This delivers a wider field of view at the same computational cost. \n",
    "* Dilated convolutions are particularly popular in the field of real-time segmentation. \n",
    "* Use them if you need a wide field of view and cannot afford multiple convolutions or larger kernels (due to computational costs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: A dilated convolution with a dilation rate of 2 and no padding\n",
    "* Blue maps are inputs and cyan maps are outputs\n",
    "<img src = '../pics/dilatedconv3x3rate2nopad.gif'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transposed Convolution (sometimes called deconvolution or fractionally strided convolution)\n",
    "\n",
    "* A transposed convolution produces the same spatial resolution a hypothetical deconvolutional layer would. \n",
    "* However, the actual mathematical operation that’s being performed on the values is different. \n",
    "* A transposed convolutional layer carries out a regular convolution but reverts its spatial transformation.\n",
    "\n",
    "Example:\n",
    "* An image of 5x5 is fed into a convolutional layer. \n",
    "* The stride is set to 2, the padding is deactivated and the kernel is 3x3. \n",
    "* This results in a 2x2 image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Transposed 2D convolution with no padding, stride of 2 and kernel of 3\n",
    "* Blue maps are inputs and cyan maps are outputs\n",
    "<img src='../pics/transposedconv3x3stride2nopad.gif'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If we wanted to reverse this process, we’d need the inverse mathematical operation so that 9 values are generated from each pixel we input. \n",
    "* Afterward, we would traverse the output image with a stride of 2. This would be a deconvolution.\n",
    "\n",
    "* A transposed convolution does not do that. The only thing in common is it guarantees that the output will be a 5x5 image as well, while still performing a normal convolution operation. To achieve this, we need to perform some fancy padding on the input.\n",
    "\n",
    "* As you can imagine now, this step will not reverse the process from above. We could get the same shape back, but not the same numeric values.\n",
    "* Instead, this would just reconstruct the spatial resolution from before and performs a convolution. \n",
    "* This may not be the mathematical inverse, but for Encoder-Decoder architectures, it’s still very helpful. This way we can combine the upscaling of an image with a convolution, instead of doing two separate processes.\n",
    "\n",
    "* Therefore, although transposed convolutions are sometimes referred to as \"deconvolutions, they shouldn't be, because they aren't actually reverting the process of a convolution. They are not the mathematical inverse of a convolutional layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separable Convolutions\n",
    "\n",
    "* In a separable convolution, we can split the kernel operation into multiple steps. \n",
    "* Let’s express a convolution as y = conv(x, k) where y is the output image, x is the input image, and k is the kernel. \n",
    "* Next, let’s assume k can be calculated by: k = k1.dot(k2). \n",
    "* This would make it a separable convolution because instead of doing a 2D convolution with k, we could get to the same result by doing 2 1D convolutions with k1 and k2.\n",
    "\n",
    "* Take the Sobel kernel for example, which is often used in image processing. \n",
    "* You could get the same kernel by multiplying the vector `[1, 0, -1]` and `[1,2,1].T`. \n",
    "* This would require 6 instead of 9 parameters while doing the same operation. \n",
    "* The example above shows what’s called a \"spatial separable convolution\" which is rare in deep learning.\n",
    "* In deep learning, one can create something very similar to a spatial separable convolution by stacking a 1xN and a Nx1 kernel layer. This was recently used in an architecture called EffNet showing promising results.\n",
    "\n",
    "#### Sobel X and Y filters\n",
    "<img src = '../pics/sobelxyfilters.png'>\n",
    "\n",
    "\n",
    "* More commonly in neural networks, we use something called a \"depthwise separable convolution\". \n",
    "* This will perform a spatial convolution while keeping the channels separate and then follow this with a depthwise convolution. \n",
    "* For example, let’s say we have a 3x3 convolutional layer on 16 input channels and 32 output channels. \n",
    "* What happens in detail is that every of the 16 channels is traversed by 32 3x3 kernels resulting in 512 (16x32) feature maps. \n",
    "* Next, we merge 1 feature map out of every input channel by adding them up. Since we can do that 32 times, we get the 32 output channels we wanted.\n",
    "* For a depthwise separable convolution on the same example, we traverse the 16 channels with 1 3x3 kernel each, giving us 16 feature maps. \n",
    "* Now, before merging anything, we traverse these 16 feature maps with 32 1x1 convolutions each and only then start to them add together. \n",
    "* This results in 656 (16x3x3 + 16x32x1x1) parameters opposed to the 4608 (16x32x3x3) parameters from above.\n",
    "* The example is a specific implementation of a depthwise separable convolution where the so called depth multiplier is 1. This is by far the most common setup for such layers.\n",
    "* We do this because of the hypothesis that spatial and depthwise information can be decoupled. Looking at the performance of the Xception model this theory seems to work. \n",
    "* Depthwise separable convolutions are also used for mobile devices because of their efficient use of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Receptive Field and Feature Map Visualization\n",
    "* The receptive field is defined as the region in the input space that a particular CNN’s feature is looking at (i.e. be affected by).\n",
    "* For convolutional neural network, the number of output features in each dimension can be calculated by the following formula:\n",
    "$$n_{out} = \\Big[\\frac{n_{in} + 2p - k}{s}\\Big]+1$$\n",
    "$$$$\n",
    "\n",
    "* $n_{in}$: number of features\n",
    "* $n_{out}$: number of output features\n",
    "* $k$: convolution kernel size\n",
    "* $p$: convolution padding size\n",
    "* $s$: convolution stride size\n",
    "\n",
    "For the moment, we'll assume that the number of (input/output) features equals the amount of the features along one axis (one dimension) of the input/output where an axis can be understood as the width, height or a channel of a color image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Feature Maps\n",
    "\n",
    "<img src='../pics/visualize_feature_map.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Left Column:** \n",
    "* The input image is a 5 x 5 matrix (blue grid). \n",
    "* Then zero-padding with size of p = 1 (transparent grid around the input image) is used to maintain the edge information during convolution. \n",
    "* After that, a 3 x 3 kernel with stride of s = 2 is used to convolve this image to obtain its feature map (green grid) with size of 3 x 3. \n",
    "* In this example, nine features are obtained and each feature has a receptive field of 3 x 3 (the area inside light blue lines). \n",
    "* We can use the same convolution on this green grid to gain a deeper feature map (orange grid) as shown in sub-figure at the left bottom. As for orange feature map, each feature has a 7 x 7 receptive field.\n",
    "* But if we only look at the feature map (green or orange grid), we cannot directly know which pixels a feature is looking at and how big that region is. \n",
    "$$$$\n",
    "<img src='../pics/visualize_feature_map.png'>\n",
    "**Right Column:**\n",
    "* In the right column, we have encoded the stride into the feature map.\n",
    "* Thus, the size of each feature map is fixed and equals the size of the input.\n",
    "* Also, each feature is located at the center of its receptive field. \n",
    "* So in this situation, its easier to see the receptive field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Receptive Field Arithmetic\n",
    "\n",
    "The receptive field for a given kernel in a particular layer can be calculated as follows\n",
    "$$$$\n",
    "* First, we calculate the number of output features in each dimension as above:\n",
    "\n",
    "$$ n_{out} = \\Big[\\frac{n_{in}+2p-k}{s}\\Big]+1$$\n",
    "\n",
    "\n",
    "* Then, we calculate the _jump_ $j$ in the output feature map. The _jump_ is the distance between two adjacent features. For the original input image, _jump_ is equal to 1.\n",
    "$$ j_{out} = j_{in}*s$$\n",
    "\n",
    "\n",
    "* Now we calculate the _size of the receptive field_ $r$ of one output feature.\n",
    "\n",
    "$$ r_{out} = r_{in}+(k-1)*j_{in}$$\n",
    "\n",
    "\n",
    "* Finally, we calculate the _center position_ of the receptive field of the first output feature.\n",
    "* Here, _start_ is the center coordinate of one pixel.\n",
    "\n",
    "$$ start_{out} = start_{in}+\\Big(\\frac{k-1}{s}-p\\Big)*j_{in}$$\n",
    "\n",
    "<img src='../pics/receptive_field_computation.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some code to generate the receptive field of any given neuron\n",
    "* Assumed layer dimensions: `[filter size, stride, padding]`\n",
    "* Assume the two filter dimensions are the same\n",
    "* Each kernel requires the following parameters:\n",
    "    - k_i: kernel size\n",
    "    - s_i: stride\n",
    "    - p_i: padding (if padding is uneven, right padding will higher than left padding; \"SAME\" option in tensorflow)\n",
    "\n",
    "* Each layer i requires the following parameters to be fully represented: \n",
    "    - n_i: number of feature (data layer has n_1 = imagesize )\n",
    "    - j_i: distance (projected to image pixel distance) between center of two adjacent features\n",
    "    - r_i: receptive field of a feature in layer i\n",
    "    - start_i: position of the first feature's receptive field in layer i (idx start from 0, negative means the center fall into padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Net summary------\n",
      "input image:\n",
      "\t n features: 227 \n",
      " \t jump: 1 \n",
      " \t receptive size: 1 \t start: 0.5 \n",
      "conv1:\n",
      "\t n features: 55 \n",
      " \t jump: 4 \n",
      " \t receptive size: 11 \t start: 5.5 \n",
      "pool1:\n",
      "\t n features: 27 \n",
      " \t jump: 8 \n",
      " \t receptive size: 19 \t start: 9.5 \n",
      "conv2:\n",
      "\t n features: 27 \n",
      " \t jump: 8 \n",
      " \t receptive size: 51 \t start: 9.5 \n",
      "pool2:\n",
      "\t n features: 13 \n",
      " \t jump: 16 \n",
      " \t receptive size: 67 \t start: 17.5 \n",
      "conv3:\n",
      "\t n features: 13 \n",
      " \t jump: 16 \n",
      " \t receptive size: 99 \t start: 17.5 \n",
      "conv4:\n",
      "\t n features: 13 \n",
      " \t jump: 16 \n",
      " \t receptive size: 131 \t start: 17.5 \n",
      "conv5:\n",
      "\t n features: 13 \n",
      " \t jump: 16 \n",
      " \t receptive size: 163 \t start: 17.5 \n",
      "pool5:\n",
      "\t n features: 6 \n",
      " \t jump: 32 \n",
      " \t receptive size: 195 \t start: 33.5 \n",
      "fc6-conv:\n",
      "\t n features: 1 \n",
      " \t jump: 32 \n",
      " \t receptive size: 355 \t start: 113.5 \n",
      "fc7-conv:\n",
      "\t n features: 1 \n",
      " \t jump: 32 \n",
      " \t receptive size: 355 \t start: 113.5 \n",
      "------------------------\n",
      "Layer name where the feature in: pool5\n",
      "index of the feature in x dimension (from 0)3\n",
      "index of the feature in y dimension (from 0)3\n",
      "receptive field: (195, 195)\n",
      "center: (129.5, 129.5)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "convnet =   [[11,4,0],[3,2,0],[5,1,2],[3,2,0],[3,1,1],[3,1,1],[3,1,1],[3,2,0],[6,1,0], [1, 1, 0]]\n",
    "layer_names = ['conv1','pool1','conv2','pool2','conv3','conv4','conv5','pool5','fc6-conv', 'fc7-conv']\n",
    "imsize = 227\n",
    "\n",
    "def outFromIn(conv, layerIn):\n",
    "  n_in = layerIn[0]\n",
    "  j_in = layerIn[1]\n",
    "  r_in = layerIn[2]\n",
    "  start_in = layerIn[3]\n",
    "  k = conv[0]\n",
    "  s = conv[1]\n",
    "  p = conv[2]\n",
    "  \n",
    "  n_out = math.floor((n_in - k + 2*p)/s) + 1\n",
    "  actualP = (n_out-1)*s - n_in + k \n",
    "  pR = math.ceil(actualP/2)\n",
    "  pL = math.floor(actualP/2)\n",
    "  \n",
    "  j_out = j_in * s\n",
    "  r_out = r_in + (k - 1)*j_in\n",
    "  start_out = start_in + ((k-1)/2 - pL)*j_in\n",
    "  return n_out, j_out, r_out, start_out\n",
    "  \n",
    "def printLayer(layer, layer_name):\n",
    "  print(layer_name + \":\")\n",
    "  print(\"\\t n features: %s \\n \\t jump: %s \\n \\t receptive size: %s \\t start: %s \" % (layer[0], layer[1], layer[2], layer[3]))\n",
    " \n",
    "layerInfos = []\n",
    "if __name__ == '__main__':\n",
    "#first layer is the data layer (image) with n_0 = image size; j_0 = 1; r_0 = 1; and start_0 = 0.5\n",
    "  print (\"-------Net summary------\")\n",
    "  currentLayer = [imsize, 1, 1, 0.5]\n",
    "  printLayer(currentLayer, \"input image\")\n",
    "  for i in range(len(convnet)):\n",
    "    currentLayer = outFromIn(convnet[i], currentLayer)\n",
    "    layerInfos.append(currentLayer)\n",
    "    printLayer(currentLayer, layer_names[i])\n",
    "  print (\"------------------------\")\n",
    "  layer_name = input (\"Layer name where the feature in: \")\n",
    "  layer_idx = layer_names.index(layer_name)\n",
    "  idx_x = int(input (\"index of the feature in x dimension (from 0)\"))\n",
    "  idx_y = int(input (\"index of the feature in y dimension (from 0)\"))\n",
    "  \n",
    "  n = layerInfos[layer_idx][0]\n",
    "  j = layerInfos[layer_idx][1]\n",
    "  r = layerInfos[layer_idx][2]\n",
    "  start = layerInfos[layer_idx][3]\n",
    "  assert(idx_x < n)\n",
    "  assert(idx_y < n)\n",
    "  \n",
    "  print (\"receptive field: (%s, %s)\" % (r, r))\n",
    "  print (\"center: (%s, %s)\" % (start+idx_x*j, start+idx_y*j))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
