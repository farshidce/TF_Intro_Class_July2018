{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "num_epochs = 5\n",
    "total_series_length = 50000\n",
    "truncated_backprop_length = 15 # number of steps to backpropagate through time\n",
    "state_size = 4\n",
    "num_classes = 2\n",
    "echo_step = 3\n",
    "batch_size = 5\n",
    "num_batches = total_series_length//batch_size//truncated_backprop_length\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Training Data\n",
    "* Here we are just making random binary vectors.\n",
    "* Our \"label\" (i.e. the output) will be an echo of the input, shifted `echo_steps` to the right.\n",
    "* Notice the reshaping of the data into a matrix with `batch_size` rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateData():\n",
    "    x = np.array(np.random.choice(2, total_series_length, p=[0.5, 0.5]))\n",
    "    y = np.roll(x, echo_step) # roll by echo_step places to create our \"echo\" data\n",
    "    y[0:echo_step] = 0\n",
    "\n",
    "    x = x.reshape((batch_size, -1))\n",
    "    y = y.reshape((batch_size, -1))\n",
    "\n",
    "    return (x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer LSTM Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For each layer of the network, we'll need a hidden state, and a cell state.\n",
    "* Normally, the input to the next LSTM layer is the previous state for that particular layer as well as the hidden activations of the \"lower\" layer that it is stacked on top of."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><td><img src='../pics/deep_RNN.png' style=\"width: 300px\"></td><td><img src='../pics/LSTM_2layer.png' style=\"width: 600px\"></td></td></tr></table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchX_placeholder = tf.placeholder(tf.float32, [batch_size, truncated_backprop_length])\n",
    "batchY_placeholder = tf.placeholder(tf.int32, [batch_size, truncated_backprop_length])\n",
    "\n",
    "num_layers = 3\n",
    "init_state = tf.placeholder(tf.float32, [num_layers, 2, batch_size, state_size])\n",
    "\n",
    "state_per_layer_list = tf.unstack(init_state, axis=0)\n",
    "rnn_tuple_state = tuple(\n",
    "    [tf.contrib.rnn.LSTMStateTuple(state_per_layer_list[idx][0], state_per_layer_list[idx][1])\n",
    "     for idx in range(num_layers)]\n",
    ")\n",
    "\n",
    "W2 = tf.Variable(np.random.rand(state_size, num_classes),dtype=tf.float32)\n",
    "b2 = tf.Variable(np.zeros((1,num_classes)), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We want to split the batch data into adjacent time-steps\n",
    "* We do this by unstacking the columns (`axis=1`) into a Python list\n",
    "* This will allow the RNN to simultaneously train on different parts of the time series.\n",
    "* Note that we are using the convention `VARIABLENAME_series` to emphasize that the variable is a list representing a timeseries with multiple samples at each step.\n",
    "* Because training is being done simultaneously on `batch_size` different places in our time-series, this requires us to save `batch_size` number of instances of RNN states when propagating forward. \n",
    "* You can see that we are accounting for this as the `init_state` placeholder has `batch_size` number of rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass\n",
    "* Notice in the `tf.concat` operation we use to create `input_and_state_concatenated`, what we actually want to do is calculate the sum of two affine transformations:\n",
    "    * `current_input * Wa` and\n",
    "    * `current_state * Wb\n",
    "* By combining these two resulting tensors, this allows us to use only one matrix multiplication. \n",
    "* Then the bias `b` is broadcast on all samples in the batch\n",
    "<img src='../pics/RNN_no_frills_forward_pass.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Forward Pass for Multilayer LSTM using `dynamic_rnn`\n",
    "* Note that as we are now making a stacked RNN, we should not reuse the same cell for the first and the deeper layers as their inputs ar different (and thus kernel matrices are different).\n",
    "* So we create an extra `make_lstm_cell` function to create a new and different cell for each layer.\n",
    "* We are no longer splitting up our inputs and labels into a list, so we have removed the lines defining `inputs_series` and `labels_series`\n",
    "* We have also changed `tf.contrib.rnn.static_rnn` to `tf.nn.dynamic_rnn`\n",
    "* The `dynamic_rnn` function takes batch inputs of shape `[batch_size, truncated_backprop_length, input_size]`, thus the addition of a single dimension on the end, which we do using `tf.expand_dims`.\n",
    "* The output will be the last state of every layer in the network as an `LSTMStateTuple` stored in `current_state` as well as the tensor `state_series` which has a shape of `[batch_size, truncated_backprop_length, state_size]` and which contains the hidden state of the last layer across all time-steps.\n",
    "* The tensor `states_series` is reshaped to shape `[batch_size * truncated_backprop_length, state_size]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These two lines no longer needed with dynamic_rnn\n",
    "# inputs_series = tf.split(batchX_placeholder, truncated_backprop_length, axis=1)\n",
    "# labels_series = tf.unstack(batchY_placeholder, axis=1)\n",
    "\n",
    "# Forward passes\n",
    "def make_lstm_cell(state_size):\n",
    "    return tf.contrib.rnn.LSTMCell(state_size, state_is_tuple=True)\n",
    "cell = tf.contrib.rnn.MultiRNNCell([make_lstm_cell(state_size) for _ in range(num_layers)], state_is_tuple=True)\n",
    "states_series, current_state = tf.nn.dynamic_rnn(cell, tf.expand_dims(batchX_placeholder, -1), initial_state=rnn_tuple_state)\n",
    "states_series = tf.reshape(states_series, [-1, state_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Side note: What is the main difference between `dynamic_rnn` vs `static_rnn` ???\n",
    "* `dynamic_rnn` supports dynamic maximum sequence length at the batch level, while `static_rnn` doesnâ€™t. \n",
    "* When running dynamic_rnn, it is assumed that the inputs' `max_time` dimension is the maximum number of frames in your minibatch, and can vary from step to step. \n",
    "* In contrast, when feeding `static_rnn` you must always provide a fixed number of inputs, which is in general longer than the maximum number of frames in your minibatch. \n",
    "* In RNNs, a **static unroll** means you have a fixed number of input tensors and you run through all of them. \n",
    "* On the other hand, **dynamic unroll** in RNNs means you stop calculating the RNNCell past the maximum `sequence_length` value, and fill in the rest of the outputs with zeros.\n",
    "* Early stopping in RNNs relies on `sequence_length`, but is wasteful in `dynamic_rnn`. \n",
    "* `dynamic_rnn` relies on the fact that your input shape can vary from step to step to perform true dynamic stopping. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating loss\n",
    "* This is the final part of the graph: a fully connected softmax layer from the state to the output that will make the classes one-hot encoded. \n",
    "* We then calculate the loss of the batch.\n",
    "* `sparse_softmax_cross_entropy_with_logits` automatically calculates the softmax internaly and then computes the cross-entropy. \n",
    "* In this example, our classes are mutually exclusive (they are either `0` or `1`), thus we use **sparse** softmax.\n",
    "* The logits should be of shape `[batchsize, num_classes]` and the labels of shape `[batch_size]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifications for working with `dynamic_rnn`\n",
    "* Now our `logits` and `labels` are tensors, and no longer Python lists.\n",
    "* In `predictions_series` we actually split the tensors into lists again, simply because the plot function we defined below is expecting a list.\n",
    "* Now, in our `losses` calculation, `sparse_softmax_cross_entropy_with_logits` can directly take our tensors without creating a special list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = tf.matmul(states_series, W2) + b2 # Broadcasted addition\n",
    "labels = tf.reshape(batchY_placeholder, [-1])\n",
    "\n",
    "# Previous version\n",
    "#logits_series = [tf.matmul(state, W2) + b2 for state in states_series] #Broadcasted addition\n",
    "#predictions_series = [tf.nn.softmax(logits) for logits in logits_series]\n",
    "\n",
    "# New version\n",
    "logits_series = tf.unstack(tf.reshape(logits, [batch_size, truncated_backprop_length, num_classes]), axis=1)\n",
    "predictions_series = [tf.nn.softmax(logit) for logit in logits_series]\n",
    "\n",
    "# Previous version\n",
    "#losses = [tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels) for logits, labels in zip(logits_series,labels_series)]\n",
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n",
    "\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "\n",
    "train_step = tf.train.AdagradOptimizer(0.3).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Side note: What is `truncated_backprop_length` ???\n",
    "* When a RNN is trained, it is actually treated as a deep neural network with reoccurring weights in every layer. \n",
    "* These layers will not be unrolled to the beginning of time as that would be too computationally expensive.\n",
    "* Therefore, the layers are truncated at a limited number of time-steps. In the diagram above, the error is backpropagated three steps in our batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "save_dir = 'logs/RNN_checkpoints'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "save_path = os.path.join(save_dir, 'best_validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(loss_list, predictions_series, batchX, batchY,dynamic=False):\n",
    "    plt.subplot(2, 3, 1)\n",
    "    if dynamic:\n",
    "        plt.gca()\n",
    "    plt.cla()\n",
    "    plt.plot(loss_list)\n",
    "\n",
    "    for batch_series_idx in range(5):\n",
    "        one_hot_output_series = np.array(predictions_series)[:, batch_series_idx, :]\n",
    "        single_output_series = np.array([(1 if out[0] < 0.5 else 0) for out in one_hot_output_series])\n",
    "\n",
    "        plt.subplot(2, 3, batch_series_idx + 2)\n",
    "        plt.cla()\n",
    "        plt.axis([0, truncated_backprop_length, 0, 2])\n",
    "        left_offset = range(truncated_backprop_length)\n",
    "        plt.bar(left_offset, batchX[batch_series_idx, :], width=1, color=\"blue\")\n",
    "        plt.bar(left_offset, batchY[batch_series_idx, :] * 0.5, width=1, color=\"red\")\n",
    "        plt.bar(left_offset, single_output_series * 0.3, width=1, color=\"green\")\n",
    "\n",
    "    plt.draw()\n",
    "    if dynamic:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "        plt.pause(0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM state is different than in a basic RNN\n",
    "* LSTMs have both a \"cell state\" and a \"hidden state, so now instead of just having a `_current_state` variable, we split this into `_current_cell_state` and `_current_hidden_state`\n",
    "* Since the `current_state` returns the cell- and hidden states in a tuple, they should be separated after calculation and supplied to the placeholders in the `run`-function. Thus, in the feed_dict, we have replaced `init_state` with `cell_state` and `hidden_state`.\n",
    "* The network's state is now stored in a single tensor, so we can modify our run function from this:\n",
    "                feed_dict={\n",
    "                    batchX_placeholder:batchX,\n",
    "                    batchY_placeholder:batchY,\n",
    "                    cell_state: _current_cell_state,\n",
    "                    hidden_state: _current_hidden_state\n",
    "                    }\n",
    "\n",
    "* To this:\n",
    "                feed_dict={\n",
    "                    batchX_placeholder:batchX,\n",
    "                    batchY_placeholder:batchY,\n",
    "                    init_state: _current_state\n",
    "                    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Failed to create session.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-9635635d2749>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdynamic_plotting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, target, graph, config)\u001b[0m\n\u001b[1;32m   1561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1562\u001b[0m     \"\"\"\n\u001b[0;32m-> 1563\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1564\u001b[0m     \u001b[0;31m# NOTE(mrry): Create these on first `__enter__` to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1565\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_graph_context_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, target, graph, config)\u001b[0m\n\u001b[1;32m    631\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_NewSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m         \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: Failed to create session."
     ]
    }
   ],
   "source": [
    "dynamic_plotting = True\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    plt.ion()\n",
    "    plt.figure()\n",
    "    plt.show()\n",
    "    loss_list = []\n",
    "\n",
    "    for epoch_idx in range(num_epochs):\n",
    "        x,y = generateData()\n",
    "\n",
    "        #_current_cell_state = np.zeros((batch_size, state_size))\n",
    "        #_current_hidden_state = np.zeros((batch_size, state_size))\n",
    "        \n",
    "        _current_state = np.zeros((num_layers, 2, batch_size, state_size))\n",
    "       \n",
    "        print(\"New data, epoch\", epoch_idx)\n",
    "\n",
    "        for batch_idx in range(num_batches):\n",
    "            start_idx = batch_idx * truncated_backprop_length\n",
    "            end_idx = start_idx + truncated_backprop_length\n",
    "\n",
    "            batchX = x[:,start_idx:end_idx]\n",
    "            batchY = y[:,start_idx:end_idx]\n",
    "\n",
    "            _total_loss, _train_step, _current_state, _predictions_series = sess.run(\n",
    "                [total_loss, train_step, current_state, predictions_series],\n",
    "                feed_dict={\n",
    "                    batchX_placeholder:batchX,\n",
    "                    batchY_placeholder:batchY,\n",
    "                    init_state: _current_state\n",
    "                })\n",
    "            \n",
    "            loss_list.append(_total_loss)\n",
    "\n",
    "            if batch_idx%100 == 0:\n",
    "                if dynamic_plotting:\n",
    "                    plot_results(loss_list, _predictions_series, batchX, batchY,dynamic=True)\n",
    "                    print(\"Step\",batch_idx, \"Loss\", _total_loss)\n",
    "                else:\n",
    "                    plot_results(loss_list, _predictions_series, batchX, batchY,dynamic=False)\n",
    "                    print(\"Step\",batch_idx, \"Loss\", _total_loss)\n",
    "                print(\"Step\",batch_idx, \"Loss\", _total_loss)\n",
    "                print(\"Step\",batch_idx, \"Loss\", _total_loss)\n",
    "\n",
    "\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
