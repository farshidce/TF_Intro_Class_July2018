{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling Keras layers on Tensorflow tensors\n",
    "* In this case, we use Keras only as a syntactical shortcut to generate an op that maps some tensor(s) input to some tensor(s) output, and that's it. \n",
    "* The optimization is done via a native TensorFlow optimizer rather than a Keras optimizer. * We don't even use any Keras Model at all!\n",
    "\n",
    "* Let's start with a simple example: MNIST digits classification. We will build a TensorFlow digits classifier using a stack of Keras Dense layers (fully-connected layers).\n",
    "* We should start by creating a TensorFlow session and registering it with Keras. This means that Keras will use the session we registered to initialize all variables that it creates internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Dense, LSTM, BatchNormalization\n",
    "from keras.objectives import categorical_crossentropy\n",
    "from keras.metrics import categorical_accuracy as accuracy\n",
    "\n",
    "from keras import backend as K\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ../data/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ../data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from mnist_loader import read_data_sets\n",
    "mnist_data = read_data_sets('../data/mnist', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "\n",
    "# this placeholder will contain our input digits, as flat vectors\n",
    "img = tf.placeholder(tf.float32, shape=(None, 784))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can easily define our model in a sequential fashion, similar to what we have done in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras layers can be called on TensorFlow tensors:\n",
    "x = Dense(128, activation='relu')(img)  # fully-connected layer with 128 units and ReLU activation\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(10, activation='softmax')(x)  # output layer with 10 units and a softmax activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a placeholder for the labels, and a loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = tf.placeholder(tf.float32, shape=(None, 10))\n",
    "\n",
    "from keras.objectives import categorical_crossentropy\n",
    "loss = tf.reduce_mean(categorical_crossentropy(labels, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can train the model with a **TensorFlow** optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "# Initialize all variables\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)\n",
    "\n",
    "# Run training loop\n",
    "with sess.as_default():\n",
    "    for i in range(100):\n",
    "        batch = mnist_data.train.next_batch(50)\n",
    "        train_step.run(feed_dict={img: batch[0],\n",
    "                                  labels: batch[1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can now evaluate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.metrics import categorical_accuracy as accuracy\n",
    "\n",
    "acc_value = accuracy(labels, preds)\n",
    "with sess.as_default():\n",
    "    print(acc_value.eval(feed_dict={img: mnist_data.test.images,\n",
    "                                    labels: mnist_data.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different behaviors during training and testing\n",
    "* Some Keras layers (e.g. Dropout, BatchNormalization) behave differently at training time and testing time. \n",
    "* You can tell whether a layer uses the \"learning phase\" (train/test) by printing `layer.uses_learning_phase`, a boolean: `True` if the layer has a different behavior in training mode and test mode, `False` otherwise.\n",
    "* If your model includes such layers, then you need to specify the value of the learning phase as part of feed_dict, so that your model knows whether to apply dropout/etc or not.\n",
    "* The Keras learning phase (a scalar TensorFlow tensor) is accessible via the Keras backend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "print(K.learning_phase())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make use of the learning phase, simply pass the value \"1\" (training mode) or \"0\" (test mode) to feed_dict like this (for train mode):\n",
    "\n",
    "`train_step.run(feed_dict={x: batch[0], labels: batch[1], K.learning_phase(): 1})`\n",
    "\n",
    "For example, here we add `Dropout` layers to our previous MNIST example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Dropout, Dense\n",
    "from keras import backend as K\n",
    "from keras.objectives import categorical_crossentropy\n",
    "\n",
    "img = tf.placeholder(tf.float32, shape=(None, 784))\n",
    "labels = tf.placeholder(tf.float32, shape=(None, 10))\n",
    "\n",
    "x = Dense(128, activation='relu')(img)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "preds = Dense(10, activation='softmax')(x)\n",
    "\n",
    "loss = tf.reduce_mean(categorical_crossentropy(labels, preds))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "sess = tf.Session()\n",
    "# Initialize all variables\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)\n",
    "\n",
    "with sess.as_default():\n",
    "    for i in range(100):\n",
    "        batch = mnist_data.train.next_batch(50)\n",
    "        train_step.run(feed_dict={img: batch[0],\n",
    "                                  labels: batch[1],\n",
    "                                  K.learning_phase(): 1})\n",
    "\n",
    "acc_value = accuracy(labels, preds)\n",
    "with sess.as_default():\n",
    "    print(acc_value.eval(feed_dict={img: mnist_data.test.images,\n",
    "                                    labels: mnist_data.test.labels,\n",
    "                                    K.learning_phase(): 0}))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compatibility with name scopes, device scopes\n",
    "Keras layers and models are fully compatible with TensorFlow name scopes. For instance, consider the following code snippet.\n",
    "\n",
    "The weights of our LSTM layer will then be named `block1/mylstm_W_i`, `block1/mylstm_U_i`, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=(None, 20, 64))\n",
    "with tf.name_scope('block1'):\n",
    "    y = LSTM(32, name='mylstm')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, device scopes work as you would expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    x = tf.placeholder(tf.float32, shape=(None, 20, 64))\n",
    "    y = LSTM(32)(x)  # all ops / variables in the LSTM layer will live on GPU:0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compatibility with graph scopes\n",
    "* Any Keras layer or model that you define inside a TensorFlow graph scope will have all of its variables and operations created as part of the specified graph. \n",
    "\n",
    "* For instance, the following works as you would expect, with all of the ops / variables in the LSTM layer created as part of our graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "import tensorflow as tf\n",
    "\n",
    "my_graph = tf.Graph()\n",
    "with my_graph.as_default():\n",
    "    x = tf.placeholder(tf.float32, shape=(None, 20, 64))\n",
    "    y = LSTM(32)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compatibility with variable scopes\n",
    "* Variable sharing should be done via calling a same Keras layer (or model) instance multiple times, and **NOT via TensorFlow variable scopes**. \n",
    "* A TensorFlow variable scope will have no effect on a Keras layer or model. \n",
    "* For more information about weight sharing with Keras, please see the \"weight sharing\" section in the functional API guide: https://keras.io/getting-started/functional-api-guide/#shared-layers\n",
    "\n",
    "To summarize quickly how weight sharing works in Keras: by reusing the same layer instance or model instance, you are sharing its weights. Here's a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a Keras layer\n",
    "lstm = LSTM(32)\n",
    "\n",
    "# instantiate two TF placeholders\n",
    "x = tf.placeholder(tf.float32, shape=(None, 20, 64))\n",
    "y = tf.placeholder(tf.float32, shape=(None, 20, 64))\n",
    "\n",
    "# encode the two tensors with the *same* LSTM weights\n",
    "x_encoded = lstm(x)\n",
    "y_encoded = lstm(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting trainable weights and state updates\n",
    "* Some Keras layers (stateful RNNs and BatchNormalization layers) have internal updates that need to be run as part of each training step. \n",
    "* There are stored as a list of tensor tuples, layer.updates. You should generate assign ops for those, to be run at each training step. Here's an example:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from keras.layers import BatchNormalization\n",
    "\n",
    "layer = BatchNormalization()(x)\n",
    "\n",
    "update_ops = []\n",
    "for old_value, new_value in layer.updates:\n",
    "    update_ops.append(tf.assign(old_value, new_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if you are using a Keras model (i.e. a `Model` instance or a `Sequential` instance), `model.udpates` behaves in the same way (and collects the updates of all underlying layers in the model).\n",
    "\n",
    "In addition, in case you need to explicitly collect a layer's trainable weights, you can do so via `layer.trainable_weights` (or `model.trainable_weights`) to get a list of TensorFlow Variable instances:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from keras.layers import Dense\n",
    "\n",
    "layer = Dense(32)(x)  # instantiate and call a layer\n",
    "print(layer.trainable_weights)  # list of TensorFlow Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Keras models with TensorFlow\n",
    "#### Converting a Keras `Sequential` model for use in a TensorFlow workflow\n",
    "\n",
    "You have found a Keras Sequential model that you want to reuse in your TensorFlow project (consider, for instance, this VGG16 image classifier with pre-trained weights: https://gist.github.com/baraldilorenzo/07d7802847aaad0a35d3). How to proceed?\n",
    "\n",
    "* First of all, note that if your pre-trained weights include convolutions (layers Convolution2D or Convolution1D) that were trained with Theano, you need to flip the convolution kernels when loading the weights. \n",
    "* This is due Theano and TensorFlow implementing convolution in different ways (TensorFlow actually implements correlation, much like Caffe). \n",
    "* Here's a short guide on what you need to do in this case: https://github.com/fchollet/keras/wiki/Converting-convolution-kernels-from-Theano-to-TensorFlow-and-vice-versa\n",
    "* Though you probably don't need to worry about this too much as fewer and fewer people are using Theano these days.\n",
    "\n",
    "* Let's say that you are starting from the following Keras model, and that you want to modify so that it takes as input a specific TensorFlow tensor, `my_input_tensor`. This input tensor could be a data feeder op, for instance, or the output of a previous TensorFlow model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import InputLayer\n",
    "\n",
    "# this is our initial Keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=784))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You just need to use `keras.layers.InputLayer` to start building your Sequential model on top of a custom TensorFlow placeholder, then build the rest of the model on top:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_input_tensor = tf.placeholder(dtype=tf.float32)\n",
    "\n",
    "# this is our modified Keras model\n",
    "model = Sequential()\n",
    "model.add(InputLayer(input_tensor=custom_input_tensor,\n",
    "                     input_shape=(None, 784)))\n",
    "\n",
    "# build the rest of the model as before\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, you can call `model.load_weights(weights_file)` to load your pre-trained weights.\n",
    "\n",
    "* Then you will probably want to collect the Sequential model's output tensor.\n",
    "* You can now add new TensorFlow ops on top of output_tensor, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tensor = model.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling a Keras model on a TensorFlow tensor\n",
    "\n",
    "* A Keras model acts the same as a layer, and thus can be called on TensorFlow tensors.\n",
    "* Note: by calling a Keras model, your are reusing both its architecture and its weights.\n",
    "* When you are calling a model on a tensor, you are creating new TF ops on top of the input tensor.\n",
    "* These new ops are reusing the TF Variable instances already present in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=784))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# this works! \n",
    "x = tf.placeholder(tf.float32, shape=(None, 784))\n",
    "y = model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-GPU and distributed training\n",
    "\n",
    "#### Assigning part of a Keras model to different GPUs\n",
    "* TensorFlow device scopes are fully compatible with Keras layers and models, hence you can use them to assign specific parts of a graph to different GPUs. Here's a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    x0 = tf.placeholder(tf.float32, shape=(None, 20, 64))\n",
    "    y0 = LSTM(32)(x)  # all ops in the LSTM layer will live on GPU:0\n",
    "\n",
    "with tf.device('/gpu:1'):\n",
    "    x1 = tf.placeholder(tf.float32, shape=(None, 20, 64))\n",
    "    y1 = LSTM(32)(x)  # all ops in the LSTM layer will live on GPU:1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note that the variables created by the LSTM layers will not live on GPU: all TensorFlow variables always live on CPU independently from the device scope where they were created.\n",
    "* TensorFlow handles device-to-device variable transfer behind the scenes.\n",
    "* If you want to train multiple replicas of a same model on different GPUs, while sharing the same weights across the different replicas, you should first instantiate your model (or layers) under one device scope, then call the same model instance multiple times in different GPU device scopes, such as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 8252968025459123186\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 10909243802\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "    link {\n",
      "      device_id: 1\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "incarnation: 9404763384781821938\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:0a:00.0, compute capability: 6.1\"\n",
      ", name: \"/device:GPU:1\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 8309797684\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "    link {\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "incarnation: 7233237150910799867\n",
      "physical_device_desc: \"device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:43:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "data = np.random.normal(size=7840).reshape(-1,784)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_g = tf.global_variables_initializer()\n",
    "init_l = tf.local_variables_initializer()\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    x = tf.placeholder(tf.float32, shape=(None, 784))\n",
    "\n",
    "    # shared model living on CPU:0\n",
    "    # it won't actually be run during training; it acts as an op template\n",
    "    # and as a repository for shared variables\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, activation='relu', input_dim=784))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# replica 0\n",
    "with tf.device('/gpu:0'):\n",
    "    output_0 = model(x)  # all ops in the replica will live on GPU:0\n",
    "\n",
    "# replica 1\n",
    "with tf.device('/gpu:1'):\n",
    "    output_1 = model(x)  # all ops in the replica will live on GPU:1\n",
    "\n",
    "# merge outputs on CPU\n",
    "with tf.device('/cpu:0'):\n",
    "    preds = 0.5 * (output_0 + output_1)\n",
    "\n",
    "# we only run the `preds` tensor, so that only the two\n",
    "# replicas on GPU get run (plus the merge op on CPU)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    output_value = sess.run([preds], feed_dict={x: data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed training\n",
    "* You can trivially make use of TensorFlow distributed training by registering with Keras a TF session linked to a cluster.\n",
    "* For more information about using TensorFlow in a distributed setting, see this tutorial: https://www.tensorflow.org/deploy/distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = tf.train.Server.create_local_server()\n",
    "sess = tf.Session(server.target)\n",
    "from keras import backend as K\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting a model with TensorFlow-serving\n",
    "* TensorFlow Serving is a library for serving TensorFlow models in a production setting, developed by Google.\n",
    "* Any Keras model can be exported with TensorFlow-serving (as long as it only has one input and one output, which is a limitation of TF-serving), whether or not it was training as part of a TensorFlow workflow. \n",
    "* In fact you could even train your Keras model with Theano then switch to the TensorFlow Keras backend and export your model.\n",
    "\n",
    "Here's how it works.\n",
    "* If your graph makes use of the Keras learning phase (different behavior at training time and test time), the very first thing to do before exporting your model is to hard-code the value of the learning phase (as 0, presumably, i.e. test mode) into your graph.\n",
    "* This is done by 1) registering a constant learning phase with the Keras backend, and 2) re-building your model afterwards.\n",
    "\n",
    "Here are these two simple steps in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "K.set_learning_phase(0)  # all new operations will be in test mode from now on\n",
    "\n",
    "previous_model=model\n",
    "\n",
    "# serialize the model and get its weights, for quick re-building\n",
    "config = previous_model.get_config()\n",
    "weights = previous_model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You called `set_weights(weights)` on layer \"dense_1\" with a  weight list of length 4, but the layer was expecting 0 weights. Provided weights: [array([[-0.0514676 , -0.0694208 ,  0.03198149, .....",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-14e4f1e74f7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel_from_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnew_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_from_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnew_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mset_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m   1060\u001b[0m                              \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m                              \u001b[0;34m' weights. Provided weights: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1062\u001b[0;31m                              str(weights)[:50] + '...')\n\u001b[0m\u001b[1;32m   1063\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: You called `set_weights(weights)` on layer \"dense_1\" with a  weight list of length 4, but the layer was expecting 0 weights. Provided weights: [array([[-0.0514676 , -0.0694208 ,  0.03198149, ....."
     ]
    }
   ],
   "source": [
    "# re-build a model where the learning phase is now hard-coded to 0\n",
    "from keras.models import model_from_config\n",
    "new_model = model_from_config(config[0])\n",
    "new_model.set_weights(weights=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use TensorFlow-serving to export the model, following the instructions found in the official tutorial: https://github.com/tensorflow/serving/blob/master/tensorflow_serving/g3doc/serving_basic.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_serving.session_bundle import exporter\n",
    "\n",
    "export_path = ... # where to save the exported graph\n",
    "export_version = ... # version number (integer)\n",
    "\n",
    "saver = tf.train.Saver(sharded=True)\n",
    "model_exporter = exporter.Exporter(saver)\n",
    "signature = exporter.classification_signature(input_tensor=model.input,\n",
    "                                              scores_tensor=model.output)\n",
    "model_exporter.init(sess.graph.as_graph_def(),\n",
    "                    default_graph_signature=signature)\n",
    "model_exporter.export(export_path, tf.constant(export_version), sess)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
