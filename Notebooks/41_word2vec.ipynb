{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "* Any feed-forward neural network that takes words from a vocabulary as input and embeds them as vectores into a lower-dimensional space, which it then fine-tunes through back-propagation, necessarily yields word embeddings as the weights of the first layer, which is usually referred to as the __Embedding Layer__.\n",
    "* Instead of doing this as a by-product of training, , the generation of word embeddings is the explicit goal of this process.\n",
    "* The hope is to produce word embeddings that encode meaningful relationships so that these embeddings can be used by other models, instead of being haphazardly or idiosyncratically linked to one specific model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec: The skip-gram model\n",
    "* We will train a simple neural network with a single hidden layer\n",
    "* Basic idea: Predict context words using center word\n",
    "<img src=\"../pics/word2vec_training_data.png\">\n",
    "* And once we've trained it, we're not going to use the model for the task it was trained on (to predict context words), but instead use the model's weights as \"word vectors\" which are (hopefully) more meaningful representations of the word's meaning.\n",
    "* It is possible to use **negative sampling** as the training method. Negative sampling is a simplified version of Noise Contrastive Estimation that makes certain assumptions about the number of noise samples to generate (k) and the distribution of noise samples (Q) - namely that kQ(w) = 1\n",
    "* TODO: bring in explanation\n",
    "* But negative sampling doesn't have the theoretical guarantee that its derivative tends towards the gradient of the softmax function. \n",
    "* NCE, on the other hand, has nice theoretical guarantees.\n",
    "* Whether its negative sampling or NCE, this is only useful at training time.\n",
    "* During inference, the full softmax still needs to be computed to obtain a normalized probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise Contrastive Estimation\n",
    "#### Some issues learning word vectors using a \"standard\" neural network.\n",
    "* Word vectors are learned while the network learns to predict the next word given a window of words (the input of the network).\n",
    "* Predicting the next word is like predicting the class. \n",
    "* That is, such a network is just a \"standard\" multinomial (multi-class) classifier. \n",
    "* The problem is, the network must have as many output neurons as classes there are. \n",
    "* When the classes are actual words, the number of neurons is huge.\n",
    "* A \"standard\" neural network is usually trained with a cross-entropy cost function which requires the values of the output neurons to represent probabilities - which means that the output \"scores\" computed by the network for each class have to be normalized and converted into actual probabilities for each class. \n",
    "* This normalization step is typically achieved using the softmax function, but softmax is very costly when applied to a huge output layer.\n",
    "\n",
    "#### The Noise Contrastive Estimation (NCE) solution\n",
    "* To deal with the expensive computation of the softmax, Word2Vec uses a technique called noise-contrastive estimation. \n",
    "* The basic idea is to convert a multinomial classification problem (predicting the next word is also multinomial classification) to a binary classification problem. \n",
    "* So, instead of using softmax to estimate a true probability distribution of the output word, a binary logistic regression (binary classification) is used instead.\n",
    "* For each training sample, the optimized classifier is fed a true pair (a center word and another word that appears in its context) and a number of randomly corrupted pairs (consisting of the center word and a randomly chosen word from the vocabulary). \n",
    "* By learning to distinguish the true pairs from corrupted ones, the classifier ultimately learns the word vectors.\n",
    "* This is important: instead of predicting the next word (the \"standard\" training technique), the optimized classifier simply predicts whether a pair of words is good or bad.\n",
    "* Word2Vec slightly customizes the process and calls it negative sampling. \n",
    "* In Word2Vec, the words for the negative samples (used for the corrupted pairs) are drawn from a specially designed distribution, which favours less frequent words to be drawn more often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "import tensorflow as tf\n",
    "\n",
    "import word2vec_utils\n",
    "\n",
    "# Model hyperparameters\n",
    "VOCAB_SIZE = 50000\n",
    "BATCH_SIZE = 128\n",
    "EMBED_SIZE = 128            # dimension of the word embedding vectors\n",
    "SKIP_WINDOW = 1             # the context window\n",
    "NUM_SAMPLED = 64            # number of negative examples to sample\n",
    "LEARNING_RATE = 1.0\n",
    "NUM_TRAIN_STEPS = 100000\n",
    "VISUAL_FLD = 'visualization'\n",
    "SKIP_STEP = 5000\n",
    "\n",
    "# Parameters for downloading data\n",
    "DOWNLOAD_URL = 'http://mattmahoney.net/dc/text8.zip'\n",
    "EXPECTED_BYTES = 31344016\n",
    "NUM_VISUALIZE = 3000        # number of tokens to visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assemble The Graph\n",
    "### Define placeholders for input and output\n",
    "### Define the weight variables\n",
    "### Define the inference model\n",
    "### Define the loss function and optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define placeholders\n",
    "* Input is the center word\n",
    "* Output is the target (context) word.\n",
    "* Instead of using one-hot vectors, we input the index of those context words directly.\n",
    "* Each sample input is a scalar, and the placeholder for BATCH_SIZE samples will thus have a shape of `[BATCH_SIZE]`.\n",
    "* Note that our `center_words` and `target_words` are both being fed in as scalars - that is as their corresponding indices in our vocabulary.\n",
    "* In this particular instance, we are feeding in a tensorflow `tf.Dataset` object, and then creating an iterator for it using `dataset.make_inializable_iterator()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the weights (in this case, embedding matrix)\n",
    "* In word2vec, the weights are actually the main things that we care about.\n",
    "* Each row corresponds to the representation vector for one word.\n",
    "* For a given word represented by a vector of EMBED_SIZE size, the the embedding matrix will have a shape of [VOCAB_SIZE, EMBED_SIZE].\n",
    "* The following code simply sets up the weight matrix for our word embeddings:\n",
    "\n",
    "**`embed_matrix = tf.get_variable('embed_matrix', \n",
    "    shape=[VOCAB_SIZE, EMBED_SIZE], \n",
    "    initializer=tf.random_uniform_initializer())`**\n",
    "\n",
    "* We initialize the embedding matrix with values from a random distribution - in this case we are using a uniform distribution.\n",
    "* Initialization is done by using the `initializer` parameter in `tf.get_variable`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the inference model\n",
    "* Our goal is to get the vector representations of words in our dictionary.\n",
    "* Remember that the embed_matrix has dimension VOCAB_SIZE x EMBED_SIZE, with each row of the embedding matrix corresponding to the vector representation of the word at that index.\n",
    "* So to get the representation of all the center words in the batch, we get a slice of all corresponding rows in the embedding matrix.\n",
    "* TensorFlow provides a convenient method to do so called `tf.nn.embedding_lookup()`\n",
    "* This method is really useful when it comes to matrix multiplication with one-hot vectors because it allows you to just do the computations that matter, avoiding the unneccessary multiplications with the zero entries in the one-hot vectors.\n",
    "* So this line gives us the embedding (or vector representation) of the center words we input:\n",
    "**`embed = tf.nn.embedding_lookup(embed_matrix, center_words, name='embedding')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the loss function and optimizer\n",
    "* First we construct the variables for NCE loss as follows.\n",
    "* These are the weights and biases for the hidden layer used to calculate the NCE loss.\n",
    "\n",
    "`nce_weight = tf.get_variable('nce_weight', shape=[VOCAB_SIZE, EMBED_SIZE],\n",
    "    initializer=tf.truncated_normal_initializer(stddev=1.0 / EMBED_SIZE ** 0.5)))`\n",
    "\n",
    "`nce_bias = tf.get_variable('nce_bias', initializer=tf.zeros([VOCAB_SIZE]))`\n",
    "\n",
    "* We then define our loss function using TensorFlow's built-in `tf.nn.nce_loss` function:\n",
    "\n",
    "**`loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weight, \n",
    "                            biases=nce_bias,\n",
    "                            labels=target_words, \n",
    "                            inputs=embed, \n",
    "                            num_sampled=NUM_SAMPLED, \n",
    "                            num_classes=VOCAB_SIZE), name='loss')`**\n",
    "                            \n",
    "* Then for our optimizer we use gradient descent:\n",
    "\n",
    "`optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(loss)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec(dataset):\n",
    "    \"\"\" Build the graph for word2vec model and train it \"\"\"\n",
    "    # Step 1: get input, output from the dataset\n",
    "    with tf.name_scope('data'):\n",
    "        iterator = dataset.make_initializable_iterator()\n",
    "        center_words, target_words = iterator.get_next()\n",
    "\n",
    "    \"\"\" Step 2: define weights \"\"\"\n",
    "    with tf.name_scope('embed'):\n",
    "        embed_matrix = tf.get_variable('embed_matrix', \n",
    "                                        shape=[VOCAB_SIZE, EMBED_SIZE],\n",
    "                                        initializer=tf.random_uniform_initializer())\n",
    "        \n",
    "        \n",
    "        \"\"\" Step 3: Inference (compute the forward path of the graph)\"\"\"\n",
    "        embed = tf.nn.embedding_lookup(embed_matrix, center_words, name='embedding')\n",
    "\n",
    "    # Step 4: construct variables for NCE loss and define loss function\n",
    "    with tf.name_scope('loss'):\n",
    "        nce_weight = tf.get_variable('nce_weight', shape=[VOCAB_SIZE, EMBED_SIZE],\n",
    "                        initializer=tf.truncated_normal_initializer(stddev=1.0 / (EMBED_SIZE ** 0.5)))\n",
    "        nce_bias = tf.get_variable('nce_bias', initializer=tf.zeros([VOCAB_SIZE]))\n",
    "\n",
    "        # define loss function to be NCE loss function\n",
    "        loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weight, \n",
    "                                            biases=nce_bias, \n",
    "                                            labels=target_words, \n",
    "                                            inputs=embed, \n",
    "                                            num_sampled=NUM_SAMPLED, \n",
    "                                            num_classes=VOCAB_SIZE), name='loss')\n",
    "\n",
    "    # Step 5: define optimizer\n",
    "    with tf.name_scope('optimizer'):\n",
    "        optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(loss)\n",
    "    \n",
    "    word2vec_utils.safe_mkdir('logs/checkpoints')\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(iterator.initializer)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        total_loss = 0.0 # we use this to calculate late average loss in the last SKIP_STEP steps\n",
    "        writer = tf.summary.FileWriter('logs/graphs/word2vec_simple', sess.graph)\n",
    "\n",
    "        for index in range(NUM_TRAIN_STEPS):\n",
    "            try:\n",
    "                loss_batch, _ = sess.run([loss, optimizer])\n",
    "                total_loss += loss_batch\n",
    "                if (index + 1) % SKIP_STEP == 0:\n",
    "                    print('Average loss at step {}: {:5.1f}'.format(index, total_loss / SKIP_STEP))\n",
    "                    total_loss = 0.0\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                sess.run(iterator.initializer)\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen():\n",
    "    yield from word2vec_utils.batch_gen(DOWNLOAD_URL, EXPECTED_BYTES, VOCAB_SIZE, \n",
    "                                        BATCH_SIZE, SKIP_WINDOW, VISUAL_FLD)\n",
    "\n",
    "def main():\n",
    "    dataset = tf.data.Dataset.from_generator(gen, \n",
    "                                (tf.int32, tf.int32), \n",
    "                                (tf.TensorShape([BATCH_SIZE]), \n",
    "                                 tf.TensorShape([BATCH_SIZE, 1])))\n",
    "    word2vec(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://mattmahoney.net/dc/text8.zip\n",
      "Successfully downloaded data/text8.zip\n",
      "Read tokens. There are 17005207 tokens\n",
      "Average loss at step 4999:  65.2\n",
      "Average loss at step 9999:  18.6\n",
      "Average loss at step 14999:   9.6\n",
      "Average loss at step 19999:   6.7\n",
      "Average loss at step 24999:   5.6\n",
      "Average loss at step 29999:   5.2\n",
      "Average loss at step 34999:   5.0\n",
      "Average loss at step 39999:   4.9\n",
      "Average loss at step 44999:   4.8\n",
      "Average loss at step 49999:   4.8\n",
      "Average loss at step 54999:   4.7\n",
      "Average loss at step 59999:   4.7\n",
      "Average loss at step 64999:   4.7\n",
      "Average loss at step 69999:   4.7\n",
      "Average loss at step 74999:   4.6\n",
      "Average loss at step 79999:   4.7\n",
      "Average loss at step 84999:   4.7\n",
      "Average loss at step 89999:   4.7\n",
      "Average loss at step 94999:   4.6\n",
      "Average loss at step 99999:   4.6\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
