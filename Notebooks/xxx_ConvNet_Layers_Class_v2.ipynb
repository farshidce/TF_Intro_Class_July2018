{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "import time \n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_relu(inputs, filters, k_size, stride, padding, scope_name):\n",
    "    '''\n",
    "    A method that does convolution + relu on inputs\n",
    "    '''\n",
    "    with tf.variable_scope(scope_name, reuse=tf.AUTO_REUSE) as scope:\n",
    "        in_channels = inputs.shape[-1]\n",
    "        kernel = tf.get_variable('kernel', \n",
    "                                [k_size, k_size, in_channels, filters], \n",
    "                                initializer=tf.truncated_normal_initializer())\n",
    "        biases = tf.get_variable('biases', \n",
    "                                [filters],\n",
    "                                initializer=tf.random_normal_initializer())\n",
    "        conv = tf.nn.conv2d(inputs, kernel, strides=[1, stride, stride, 1], padding=padding)\n",
    "    return tf.nn.relu(conv + biases, name=scope.name)\n",
    "\n",
    "def maxpool(inputs, ksize, stride, padding='VALID', scope_name='pool'):\n",
    "    '''A method that does max pooling on inputs'''\n",
    "    with tf.variable_scope(scope_name, reuse=tf.AUTO_REUSE) as scope:\n",
    "        pool = tf.nn.max_pool(inputs, \n",
    "                            ksize=[1, ksize, ksize, 1], \n",
    "                            strides=[1, stride, stride, 1],\n",
    "                            padding=padding)\n",
    "    return pool\n",
    "\n",
    "def fully_connected(inputs, out_dim, scope_name='fc'):\n",
    "    '''\n",
    "    A fully connected linear layer on inputs\n",
    "    '''\n",
    "    with tf.variable_scope(scope_name, reuse=tf.AUTO_REUSE) as scope:\n",
    "        in_dim = inputs.shape[-1]\n",
    "        w = tf.get_variable('weights', [in_dim, out_dim],\n",
    "                            initializer=tf.truncated_normal_initializer())\n",
    "        b = tf.get_variable('biases', [out_dim],\n",
    "                            initializer=tf.constant_initializer(0.0))\n",
    "        out = tf.matmul(inputs, w) + b\n",
    "    return out\n",
    "\n",
    "class ConvNet(object):\n",
    "    def __init__(self):\n",
    "        self.lr = 0.001\n",
    "        self.batch_size = 128\n",
    "        self.keep_prob = tf.constant(0.75)\n",
    "        self.gstep = tf.Variable(0, dtype=tf.int32, \n",
    "                                trainable=False, name='global_step')\n",
    "        self.n_classes = 10\n",
    "        self.skip_step = 20\n",
    "        self.n_test = 10000\n",
    "        self.training = True\n",
    "\n",
    "    def get_data(self):\n",
    "        with tf.name_scope('data'):\n",
    "            train_data, test_data = data.get_mnist_dataset(self.batch_size)\n",
    "            iterator = tf.data.Iterator.from_structure(train_data.output_types, \n",
    "                                                   train_data.output_shapes)\n",
    "            img, self.label = iterator.get_next()\n",
    "            self.img = tf.reshape(img, shape=[-1, 28, 28, 1])\n",
    "            # reshape the image to make it work with tf.nn.conv2d\n",
    "\n",
    "            self.train_init = iterator.make_initializer(train_data)  # initializer for train_data\n",
    "            self.test_init = iterator.make_initializer(test_data)    # initializer for train_data\n",
    "\n",
    "    def inference(self):\n",
    "        conv1 = conv_relu(inputs=self.img,\n",
    "                        filters=32,\n",
    "                        k_size=5,\n",
    "                        stride=1,\n",
    "                        padding='SAME',\n",
    "                        scope_name='conv1')\n",
    "        pool1 = maxpool(conv1, 2, 2, 'VALID', 'pool1')\n",
    "        conv2 = conv_relu(inputs=pool1,\n",
    "                        filters=64,\n",
    "                        k_size=5,\n",
    "                        stride=1,\n",
    "                        padding='SAME',\n",
    "                        scope_name='conv2')\n",
    "        pool2 = maxpool(conv2, 2, 2, 'VALID', 'pool2')\n",
    "        feature_dim = pool2.shape[1] * pool2.shape[2] * pool2.shape[3]\n",
    "        pool2 = tf.reshape(pool2, [-1, feature_dim])\n",
    "        fc = fully_connected(pool2, 1024, 'fc')\n",
    "        dropout = tf.nn.dropout(tf.nn.relu(fc), self.keep_prob, name='relu_dropout')\n",
    "        self.logits = fully_connected(dropout, self.n_classes, 'logits')\n",
    "\n",
    "    def loss(self):\n",
    "        '''\n",
    "        define loss function\n",
    "        use softmax cross entropy with logits as the loss function\n",
    "        compute mean cross entropy, softmax is applied internally\n",
    "        '''\n",
    "        # \n",
    "        with tf.name_scope('loss'):\n",
    "            entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=self.label, logits=self.logits)\n",
    "            self.loss = tf.reduce_mean(entropy, name='loss')\n",
    "    \n",
    "    def optimize(self):\n",
    "        '''\n",
    "        Define training op\n",
    "        using Adam Gradient Descent to minimize cost\n",
    "        '''\n",
    "        self.opt = tf.train.AdamOptimizer(self.lr).minimize(self.loss, \n",
    "                                                global_step=self.gstep)\n",
    "\n",
    "    def summary(self):\n",
    "        pass\n",
    "        '''\n",
    "        Create summaries to write on TensorBoard\n",
    "        '''\n",
    "#         with tf.name_scope('summaries'):\n",
    "#             tf.summary.scalar('loss', self.loss)\n",
    "#             tf.summary.scalar('accuracy', self.accuracy)\n",
    "#             tf.summary.histogram('histogram_loss', self.loss)\n",
    "#             self.summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    def eval(self):\n",
    "        '''\n",
    "        Count the number of right predictions in a batch\n",
    "        '''\n",
    "        with tf.name_scope('predict'):\n",
    "            preds = tf.nn.softmax(self.logits)\n",
    "            correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(self.label, 1))\n",
    "            self.accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "\n",
    "    def build(self):\n",
    "        '''\n",
    "        Build the computation graph\n",
    "        '''\n",
    "        self.get_data()\n",
    "        self.inference()\n",
    "        self.loss()\n",
    "        self.optimize()\n",
    "        self.eval()\n",
    "        self.summary()\n",
    "\n",
    "    def train_one_epoch(self, sess, saver, init, writer, epoch, step):\n",
    "        start_time = time.time()\n",
    "        sess.run(init) \n",
    "        self.training = True\n",
    "        total_loss = 0\n",
    "        n_batches = 0\n",
    "        try:\n",
    "            while True:\n",
    "                _, l = sess.run([self.opt, self.loss])\n",
    "                #_, l, summaries = sess.run([self.opt, self.loss, self.summary_op])\n",
    "                #writer.add_summary(summaries, global_step=step)\n",
    "                #if (step + 1) % self.skip_step == 0:\n",
    "                #    print('Loss at step {0}: {1}'.format(step, l))\n",
    "                step += 1\n",
    "                total_loss += l\n",
    "                n_batches += 1\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "        saver.save(sess, 'logs/ConvNet_Layers_Class', step)\n",
    "        print('Average loss at epoch {0}: {1}'.format(epoch, total_loss/n_batches))\n",
    "        print('Took: {0} seconds'.format(time.time() - start_time))\n",
    "        return step\n",
    "\n",
    "    def eval_once(self, sess, init, writer, epoch, step):\n",
    "        start_time = time.time()\n",
    "        sess.run(init)\n",
    "        self.training = False\n",
    "        total_correct_preds = 0\n",
    "        try:\n",
    "            while True:\n",
    "                accuracy_batch, summaries = sess.run([self.accuracy, self.summary_op])\n",
    "                writer.add_summary(summaries, global_step=step)\n",
    "                total_correct_preds += accuracy_batch\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "\n",
    "        print('Accuracy at epoch {0}: {1} '.format(epoch, total_correct_preds/self.n_test))\n",
    "        print('Took: {0} seconds'.format(time.time() - start_time))\n",
    "\n",
    "    def train(self, n_epochs):\n",
    "        '''\n",
    "        The train function alternates between training one epoch and evaluating\n",
    "        '''\n",
    "        data.safe_mkdir('logs')\n",
    "        data.safe_mkdir('logs/ConvNet_Layers_Class')\n",
    "        writer = tf.summary.FileWriter('./logs/ConvNet_Layers_Class', tf.get_default_graph())\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            saver = tf.train.Saver()\n",
    "            ckpt = tf.train.get_checkpoint_state(os.path.dirname('logs/ConvNet_Layers_Class'))\n",
    "            if ckpt and ckpt.model_checkpoint_path:\n",
    "                saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            \n",
    "            step = self.gstep.eval()\n",
    "\n",
    "            for epoch in range(n_epochs):\n",
    "                step = self.train_one_epoch(sess, saver, self.train_init, writer, epoch, step)\n",
    "                self.eval_once(sess, self.test_init, writer, epoch, step)\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/mnist/train-images-idx3-ubyte.gz already exists\n",
      "../data/mnist/train-labels-idx1-ubyte.gz already exists\n",
      "../data/mnist/t10k-images-idx3-ubyte.gz already exists\n",
      "../data/mnist/t10k-labels-idx1-ubyte.gz already exists\n",
      "Average loss at epoch 0: 2958.4657046295874\n",
      "Took: 89.09846019744873 seconds\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ConvNet' object has no attribute 'summary_op'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-8a98bbf80fbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskip_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-a0ddf73a18ab>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, n_epochs)\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-a0ddf73a18ab>\u001b[0m in \u001b[0;36meval_once\u001b[0;34m(self, sess, init, writer, epoch, step)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m                 \u001b[0maccuracy_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m                 \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummaries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mtotal_correct_preds\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0maccuracy_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ConvNet' object has no attribute 'summary_op'"
     ]
    }
   ],
   "source": [
    "model = ConvNet()\n",
    "model.skip_step = 100\n",
    "model.build()\n",
    "model.train(n_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
