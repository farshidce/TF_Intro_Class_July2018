{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization\n",
    "* Batch normalization is monstly a technique for improving optimization.\n",
    "* When you have a large dataset, it's more important to optimize well, while regularization becomes less critical as your number of samples increases.\n",
    "* Of course you can use both batch normalization and dropout at the same time.\n",
    "* BatchNorm isn't really used for regularization, as BatchNorm doesn't smooth the cost.\n",
    "* Instead, BatchNorm is added in order to improve the performance of backpropagation.\n",
    "* In essence, it keeps the back propagated gradient from getting too big or small by rescaling and recentering.\n",
    "* As a technique, it is related to second-order optimization methods that attempt to model the curvature of the cost surface.\n",
    "* BatchNorm can also be used to guarantee that the relative scaling is correct if you are going to add random noise to neural activations.\n",
    "\n",
    "#### BatchNorm vs Dropout\n",
    "* As a side effect, batch normalization also introduces some noise into the network, so it can regularize the model a little bit.\n",
    "* Why? Basically, we are multiplying network weights by a noise vector (containing ones and zeros).\n",
    "* BatchNorm is similar to dropout in the sense that it multiplies each hidden unit by a random value at each step of training. \n",
    "* In this case, the random value is the standard deviation of all the hidden units in the minibatch.\n",
    "* Because different examples are randomly chosen for inclusiion in the minibatch at each step, the standard deviation fluctuates randomly.\n",
    "* BatchNorm also subtracts a random value (the mean of the minibatch) from each hidden unit at each step.\n",
    "* Both of these sources of noise mean that every layer has to learn to be robust o a lot of variation in its input, just like with dropout.\n",
    "\n",
    "#### Why BatchNorm Works\n",
    "* After normalizing a neural network's inputs, we no longer have to worry that the scale of the input features have an extremely high variance.\n",
    "* Thus, gradient descent's oscillations are dampened when approaching a minima in the loss surface, and convergence is faster.\n",
    "* BatchNorm also reduces the impact of earlier layers on later layers in a deep neural network.\n",
    "* If we take a slice from the middle of our network, e.g. layer #10, we can see that layer 10's input features change during training.\n",
    "* This makes training more difficult, causing the model to take longer to converge.\n",
    "* BatchNorm can reduce the impact of earlier layers by keeping the mean and variance fixed, which makes the layers kind of more independent of each other.\n",
    "\n",
    "\n",
    "#### Drawbacks of BatchNorm\n",
    "* BatchNorm has a computational cost as it has two more parameters to optimize.\n",
    "* Due to the exponential moving average, if the mini-batch does not properly represent the entire data distribution, model performance could be heavily impacted.\n",
    "\n",
    "#### Covariate Shift\n",
    "* Batch norm limits the internal co-variate shift by normalizing the data over and over again.\n",
    "* So what is covariate shift and why does it matter?\n",
    "* Covariate shift is when your inputs change on you, and your algorithm can't deal with it.\n",
    "* More formally, covariate shif is a change in the distribution of a function's domain.\n",
    "* So if a net's parameters were trained on distribution A, and we give it data from a different distribution, lets say B, then the trained model will not perform very well.\n",
    "* Within a single training set, covariate shift isn't normally a problem.\n",
    "* Even if you're taking subsets/mini-batches, the statistics between batches shouldn't be off by too much, provided you've randomized your dataset.\n",
    "* But most deep learning architectures are hierarchical...\n",
    "* At the first layer, you're looking at data from dataset D and the statistics between batches remain similar during training.\n",
    "* But the first layer feeds the second layer, and the second feeds the third, etc., and once you get to layer, for example, 100 - this becomes problematic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization Worksheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 5731
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5640,
     "status": "ok",
     "timestamp": 1522178339431,
     "user": {
      "displayName": "Jae duk Seo",
      "photoUrl": "//lh5.googleusercontent.com/-U3yvHWVYfHs/AAAAAAAAAAI/AAAAAAAAAXc/AWDj51rHROw/s50-c-k-no/photo.jpg",
      "userId": "105828457776204897992"
     },
     "user_tz": 240
    },
    "id": "zh2yMYTf5TXR",
    "outputId": "f2cec8c4-c29a-44c3-b7c5-58fee0ce51b0"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "np.random.seed(678)\n",
    "tf.set_random_seed(678)\n",
    "config = tf.ConfigProto(device_count = {'GPU': 0})\n",
    "sess = tf.InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Some Random Data\n",
    "* To simulate a real world use case, we create an 32*32 image from random normal distrubition and add some noise to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 5731
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5640,
     "status": "ok",
     "timestamp": 1522178339431,
     "user": {
      "displayName": "Jae duk Seo",
      "photoUrl": "//lh5.googleusercontent.com/-U3yvHWVYfHs/AAAAAAAAAAI/AAAAAAAAAXc/AWDj51rHROw/s50-c-k-no/photo.jpg",
      "userId": "105828457776204897992"
     },
     "user_tz": 240
    },
    "id": "zh2yMYTf5TXR",
    "outputId": "f2cec8c4-c29a-44c3-b7c5-58fee0ce51b0"
   },
   "outputs": [],
   "source": [
    "global  test_data\n",
    "test_data = np.zeros((30,32,32,1))\n",
    "for i in range(30):\n",
    "    new_random_image = np.random.randn(32,32) * np.random.randint(5) + np.random.randint(60)\n",
    "    new_random_image = np.expand_dims(new_random_image,axis=2)\n",
    "    test_data[i,:,:,:] = new_random_image\n",
    "\n",
    "print('\\n===================================')\n",
    "print(\"Data Shape: \",test_data.shape, \" (# of Images, Image Width, Image Height, Channels)\")\n",
    "print(\"Data Max: \",test_data.max())\n",
    "print(\"Data Min: \",test_data.min())\n",
    "print(\"Data Mean: \",test_data.mean())\n",
    "print(\"Data Variance: \",test_data.var())\n",
    "print('===================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample two images from our data and plot them along with the data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 5731
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5640,
     "status": "ok",
     "timestamp": 1522178339431,
     "user": {
      "displayName": "Jae duk Seo",
      "photoUrl": "//lh5.googleusercontent.com/-U3yvHWVYfHs/AAAAAAAAAAI/AAAAAAAAAXc/AWDj51rHROw/s50-c-k-no/photo.jpg",
      "userId": "105828457776204897992"
     },
     "user_tz": 240
    },
    "id": "zh2yMYTf5TXR",
    "outputId": "f2cec8c4-c29a-44c3-b7c5-58fee0ce51b0"
   },
   "outputs": [],
   "source": [
    "testdata_img_1 = np.squeeze(test_data[0,:,:,:])\n",
    "testdata_img_2 = np.squeeze(test_data[4,:,:,:])\n",
    "\n",
    "f, axarr = plt.subplots(1,3,figsize=(12,4))    \n",
    "axarr[0].imshow(testdata_img_1,cmap='gray')\n",
    "axarr[1].imshow(testdata_img_2,cmap='gray')\n",
    "axarr[2].hist(test_data.flatten() ,bins='auto')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --------- Case 1 normalize the entire dataset ------\n",
    "$$\\LARGE{X_{new} = \\frac{X-X_{min}}{X_{max} - X_{min}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 5731
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5640,
     "status": "ok",
     "timestamp": 1522178339431,
     "user": {
      "displayName": "Jae duk Seo",
      "photoUrl": "//lh5.googleusercontent.com/-U3yvHWVYfHs/AAAAAAAAAAI/AAAAAAAAAXc/AWDj51rHROw/s50-c-k-no/photo.jpg",
      "userId": "105828457776204897992"
     },
     "user_tz": 240
    },
    "id": "zh2yMYTf5TXR",
    "outputId": "f2cec8c4-c29a-44c3-b7c5-58fee0ce51b0"
   },
   "outputs": [],
   "source": [
    "normdata = (test_data - test_data.min(axis=0)) / \\\n",
    "(test_data.max(axis=0) - test_data.min(axis=0))\n",
    "normdata_img_1 = np.squeeze(normdata[0,:,:,:])\n",
    "normdata_img_2 = np.squeeze(normdata[4,:,:,:])\n",
    "\n",
    "print('============== Normalized Data ==============')\n",
    "print(\"Data Shape: \",normdata.shape)\n",
    "print(\"Data Max: \",normdata.max())\n",
    "print(\"Data Min: \",normdata.min())\n",
    "print(\"Data Mean: \",normdata.mean())\n",
    "print(\"Data Variance: \",normdata.var())\n",
    "print('=============================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can now clearly see that while our images appear the same, the data now ranges between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 5731
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5640,
     "status": "ok",
     "timestamp": 1522178339431,
     "user": {
      "displayName": "Jae duk Seo",
      "photoUrl": "//lh5.googleusercontent.com/-U3yvHWVYfHs/AAAAAAAAAAI/AAAAAAAAAXc/AWDj51rHROw/s50-c-k-no/photo.jpg",
      "userId": "105828457776204897992"
     },
     "user_tz": 240
    },
    "id": "zh2yMYTf5TXR",
    "outputId": "f2cec8c4-c29a-44c3-b7c5-58fee0ce51b0"
   },
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(1,3,figsize=(12,4))    \n",
    "axarr[0].imshow(normdata_img_1,cmap='gray')\n",
    "axarr[1].imshow(normdata_img_2,cmap='gray')\n",
    "axarr[2].hist(normdata.flatten() ,bins='auto')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- case 2 Standardization: standardize the whole dataset using standard deviation ---\n",
    "$$\\LARGE{X_{new} = \\frac{X - \\mu}{\\sigma}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 5731
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5640,
     "status": "ok",
     "timestamp": 1522178339431,
     "user": {
      "displayName": "Jae duk Seo",
      "photoUrl": "//lh5.googleusercontent.com/-U3yvHWVYfHs/AAAAAAAAAAI/AAAAAAAAAXc/AWDj51rHROw/s50-c-k-no/photo.jpg",
      "userId": "105828457776204897992"
     },
     "user_tz": 240
    },
    "id": "zh2yMYTf5TXR",
    "outputId": "f2cec8c4-c29a-44c3-b7c5-58fee0ce51b0"
   },
   "outputs": [],
   "source": [
    "standdata = (test_data - test_data.mean(axis=0)) / test_data.std(axis=0)\n",
    "standdata_img_1 = np.squeeze(standdata[0,:,:,:])\n",
    "standdata_img_2 = np.squeeze(standdata[4,:,:,:])\n",
    "\n",
    "print('============== Standardized Data  ==============')\n",
    "print(\"Data Shape: \",standdata.shape)\n",
    "print(\"Data Max: \",standdata.max())\n",
    "print(\"Data Min: \",standdata.min())\n",
    "print(\"Data Mean: \",standdata.mean())\n",
    "print(\"Data Variance: \",standdata.var())\n",
    "print('================================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Following standardization, we can see that the mean of our data has shifted to around 0, and variance to 1, but visually, it still looks exactly the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(1,3,figsize=(12,4))    \n",
    "axarr[0].imshow(standdata_img_1,cmap='gray')\n",
    "axarr[1].imshow(standdata_img_2,cmap='gray')\n",
    "axarr[2].hist(standdata.flatten() ,bins='auto')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --------- case 3a batch normalize the first 10 images ------\n",
    "* **Input:** Values of $x$ over a mini-batch: $\\mathcal{B} = x_{1,...,m}$\n",
    "* Parameters to be learned: $\\large{\\gamma,\\beta}$\n",
    "* **Output:** ${y_i = BN_{\\gamma,\\beta}(x_i)}$\n",
    "\n",
    "**mini-batch mean:**\n",
    "$$\\mu\\mathcal{B}\\leftarrow\\frac{1}{m}\\sum_{i=1}^mx_i$$\n",
    "\n",
    "**mini-batch variance:**\n",
    "$$\\sigma_{\\mathcal{B}}^2\\leftarrow\\frac{1}{m}\\sum_{i=1}^m(x_i-\\mu\\mathcal{B})^2$$\n",
    "\n",
    "**normalize:** `note this is also equivalent to the equation for Standardization`\n",
    "$$\\hat x\\leftarrow\\frac{x_i-\\mu\\mathcal{B}}{\\sqrt{\\sigma_{\\mathcal{B}}^2+\\epsilon}}$$\n",
    "\n",
    "**scale and shift:**\n",
    "$$y_i\\leftarrow\\gamma\\hat x_i + \\beta \\equiv BN_{\\gamma,\\beta}(x_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 5731
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5640,
     "status": "ok",
     "timestamp": 1522178339431,
     "user": {
      "displayName": "Jae duk Seo",
      "photoUrl": "//lh5.googleusercontent.com/-U3yvHWVYfHs/AAAAAAAAAAI/AAAAAAAAAXc/AWDj51rHROw/s50-c-k-no/photo.jpg",
      "userId": "105828457776204897992"
     },
     "user_tz": 240
    },
    "id": "zh2yMYTf5TXR",
    "outputId": "f2cec8c4-c29a-44c3-b7c5-58fee0ce51b0"
   },
   "outputs": [],
   "source": [
    "first10_data = test_data[:10,:,:,:]\n",
    "\n",
    "# column-wise sums / # of samples\n",
    "mini_batch_mean = first10_data.sum(axis=0) / len(first10_data)\n",
    "mini_batch_var = ((first10_data - mini_batch_mean) ** 2).sum(axis=0) / len(first10_data)\n",
    "batchnorm_data = (first10_data - mini_batch_mean)/ ( (mini_batch_var + 1e-8) ** 0.5 )\n",
    "\n",
    "bndata_img_1 = np.squeeze(batchnorm_data[0,:,:,:])\n",
    "bndata_img_2 = np.squeeze(batchnorm_data[4,:,:,:])\n",
    "print('============== Case 3 Implementation ===================')\n",
    "print(\"Data Shape: \",batchnorm_data.shape)\n",
    "print(\"Data Max: \",batchnorm_data.max())\n",
    "print(\"Data Min: \",batchnorm_data.min())\n",
    "print(\"Data Mean: \",batchnorm_data.mean())\n",
    "print(\"Data Variance: \",batchnorm_data.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Again, the mean is around zero, and variance is around 1, and the images do not look much different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(1,3,figsize=(12,4))    \n",
    "axarr[0].imshow(bndata_img_1,cmap='gray')\n",
    "axarr[1].imshow(bndata_img_2,cmap='gray')\n",
    "axarr[2].hist(batchnorm_data.flatten() ,bins='auto')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --------- case 3b batch norm TensorFlow ------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 5731
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5640,
     "status": "ok",
     "timestamp": 1522178339431,
     "user": {
      "displayName": "Jae duk Seo",
      "photoUrl": "//lh5.googleusercontent.com/-U3yvHWVYfHs/AAAAAAAAAAI/AAAAAAAAAXc/AWDj51rHROw/s50-c-k-no/photo.jpg",
      "userId": "105828457776204897992"
     },
     "user_tz": 240
    },
    "id": "zh2yMYTf5TXR",
    "outputId": "f2cec8c4-c29a-44c3-b7c5-58fee0ce51b0"
   },
   "outputs": [],
   "source": [
    "bndataTF = tf.nn.batch_normalization(first10_data,\n",
    "                mean = first10_data.mean(axis=0),\n",
    "                variance = first10_data.var(axis=0),\n",
    "                offset = 0.0,scale = 1.0,\n",
    "                variance_epsilon = 1e-8\n",
    "                ).eval()\n",
    "\n",
    "bndataTF_img_1 = np.squeeze(bndataTF[0,:,:,:])\n",
    "bndataTF_img_2 = np.squeeze(bndataTF[4,:,:,:])\n",
    "print('============== Case 3b Tensorflow ===================')\n",
    "print(\"Data Shape: \",bndataTF.shape)\n",
    "print(\"Data Max: \",bndataTF.max())\n",
    "print(\"Data Min: \",bndataTF.min())\n",
    "print(\"Data Mean: \",bndataTF.mean())\n",
    "print(\"Data Variance: \",bndataTF.var())\n",
    "print('=================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "33 Batch Norm",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
